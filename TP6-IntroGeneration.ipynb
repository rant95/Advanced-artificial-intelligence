{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fed6705",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpintrogeneration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9cac1",
   "metadata": {},
   "source": [
    "# Travaux pratiques : introduction aux modèles génératifs\n",
    "\n",
    "L’objectif de cette séance de TP est de réaliser un bref tour d’horizon\n",
    "de trois types de modèles génératifs simples:\n",
    "\n",
    "- modèles de mélanges gaussiens,  \n",
    "- chaînes de Markov,  \n",
    "- autoencodeurs.  \n",
    "\n",
    "\n",
    "La séance est divisée en 3 exercices indépendants pouvant être traités\n",
    "dans le désordre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b2115",
   "metadata": {},
   "source": [
    "## Exercice 1 : Modèles de mélanges gaussiens\n",
    "\n",
    "Ce premier exercice est une application directe des modèles de mélanges\n",
    "gaussiens. L’objectif est d’illustrer le fonctionnement de\n",
    "l’échantillonnage de nouvelles données à partir de la densité estimée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5ad71",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca45d38",
   "metadata": {},
   "source": [
    "Pour cet exercice, nous allons travailler sur un nuage de points\n",
    "synthétique qui est généré à partir d’une sinusoïde. Les observations\n",
    "$ X $ sont bi-dimensionnelles avec:\n",
    "\n",
    "$$\n",
    "x_2 = \\sin(x_1) + \\epsilon\n",
    "$$\n",
    "\n",
    "avec:\n",
    "\n",
    "- $ x_1 $ une variable aléatoire tirée uniformément dans $ [0, 10] $,  \n",
    "- $ \\epsilon $ un bruit aléatoire tiré d’une loi normale $ \\mathcal{N}(0, 0.1) $.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1800654",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "x1 = 10 * np.random.rand(n_samples)\n",
    "x2 = np.sin(x1) + 0.1*np.random.randn(n_samples)\n",
    "X = np.vstack((x1, x2))\n",
    "X = np.transpose(X, (1, 0))\n",
    "\n",
    "# Affichage du nuage de point\n",
    "plt.scatter(X[:,0], X[:,1], s=10)\n",
    "plt.title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71352f",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Appliquer un [modèle de mélange gaussien](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) à la matrice d’observations $ X $. On choisira de façon empirique un nombre de composantes qui semble adapté au nuage de points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec820a42",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "L’idéal serait d’utiliser le critère AIC ou BIC pour déterminer le nombre de composantes. En première approximation, considérons 6 composantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a55de0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=6)\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b65c6e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def make_ellipses(gmm, ax):\n",
    "    colors = ['navy', 'firebrick', 'darkorange', 'dimgrey', 'forestgreen',\n",
    "              'purple', 'turquoise', 'yellow', 'hotpink', 'springgreen']\n",
    "\n",
    "    for n, color in enumerate(colors[:gmm.n_components]):\n",
    "        if gmm.covariance_type == 'full':\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == 'tied':\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == 'diag':\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == 'spherical':\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n",
    "                                  180 + angle, color=color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.text(*(gmm.means_[n, :2]+0.1), str(n), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd3bd4",
   "metadata": {},
   "source": [
    "Le code ci-dessous permet d’afficher les nuage de points et les ellipses\n",
    "correspondants aux gaussiennes (centrées sur la moyenne et d’axes égaux\n",
    "aux covariances) en surimpression.\n",
    "\n",
    "**Note**: il n’est pas indispensable de comprendre les détails de la\n",
    "fonction `make_ellipses` pour ce TP, celle-ci faisant appel à des\n",
    "fonctionnalités avancées de la bibliothèque graphique `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5669cf5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "make_ellipses(gmm, ax)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=3)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim((-1.5, 1.5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bbce3",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Tirer aléatoirement 200 points dans le modèle de mélange. Afficher le résultat.\n",
    "\n",
    "**Indice**: utiliser une des méthodes de l’objet [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32658f07",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "`sklearn` intègre la méthode `sample()` pour échantillonner dans le mélange :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047c9fe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X_samples = gmm.sample(200)[0]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=3, alpha=0.5)\n",
    "plt.scatter(X_samples[:, 0], X_samples[:, 1], marker='x', s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c23a0",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Tirer aléatoirement 100 points dans la première composante du mélange. Afficher le résultat.\n",
    "\n",
    "**Indice**: la moyenne et la matrice de variance-covariance de la composante numéro $ i $ du mélange sont stockées respectivement dans `gmm.means_[i]` et `gmm.covariances_[i]`. La fonction `multivariate_normal` de [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html) peut être utile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79f017",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "Chaque composante du mélange est une gaussienne multivariée dont la moyenne et la matrice de covariance sont estimées lors de l’apprentissage. Pour échantillonner selon une composante précise, il suffit donc de tirer selon cette gaussienne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d54173",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "i = 0 # numéro de la composante souhaitée\n",
    "X_samples = np.random.multivariate_normal(gmm.means_[i], gmm.covariances_[i], size=100)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=3, alpha=0.5)\n",
    "plt.scatter(X_samples[:, 0], X_samples[:, 1], marker='x', s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adcb08",
   "metadata": {},
   "source": [
    "## Exercice 2 : Autoencodeurs et génération d’images\n",
    "\n",
    "Cet exercice présente l’utilisation des autoencodeurs avec PyTorch. Nous\n",
    "allons utiliser un modèle simple, entièrement connecté, permettant de\n",
    "compresser une image en une représentation vectorielle.\n",
    "\n",
    "Cet exercice utilise MNIST (ou FashionMNIST) comme jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d4bbd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Imports liés à torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44be4c4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.FashionMNIST(\"data/\", download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb964f7",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Visualiser les premières images du jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79975506",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c835c21",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for idx, (img, label) in enumerate(mnist):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    fig.add_subplot(1,5,idx+1)\n",
    "    plt.imshow(img[0].numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90791072",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "En utilisant l’interface `nn.Sequential` de PyTorch, écrire le code qui définit un modèle autoencodeur entièrement connecté.\n",
    "\n",
    "Ce modèle prend en entrée une image de dimensions $ 28\\times28 $ sous forme d’un vecteur aplati de longeur $ 784 $. On définira une variable `hidden_state` qui permet de contrôler la taille du code $ z $ en sortie de l’encodeur.\n",
    "\n",
    "Le décodeur devra prendre en entrée un code $ z $ de longueur `hidden_state` et produire un vecteur aplati de longueur $ 28\\times28 = 784 $ (identique à l’image). On choisira une valeur raisonnable pour la dimension du code (par exemple, entre 30 et 250)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a675f4d",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "On se borne ici à un auto-encodeur « simple », c’est-à-dire une combinaison de deux perceptrons multi-couches. On pourrait aussi utiliser un encodeur convolutif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab74807",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "hidden_state = 128\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(28*28, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, hidden_state)\n",
    ")\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(hidden_state, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 28*28),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "autoencoder = nn.Sequential(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ab22e",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Y a-t-il un intérêt à utiliser une fonction de non-linéarité en sortie du décodeur? Justifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048362ae",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "Lorsque l’espace de sortie (et donc l’espace d’entrée) n’est pas bornée, il n’est généralement contre-productif d’appliquer une non-linéarité en sortie du décodeur. Par exemple, si l’on souhaite reconstruire des valeurs réelles positives et négatives, il ne faut surtout pas appliquer de ReLU (qui va interdire les valeurs négatives).\n",
    "\n",
    "Dans notre cas, les valeurs des pixels sont cependant bornées entre 0 et 1. Il est possible que le décodeur produise des valeurs légèrement négatives ou légèrement supérieures à 1, ce qui produira des artefacts dans l’image si on ne les enlève pas (*clipping*). Une solution élégante est d’appliquer une non-linéarité sigmoïde qui va contraindre les valeurs des pixels dans la plage (0,1) désirée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6954f6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=20, device=\"cpu\"):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    flatten = nn.Flatten()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
    "    for e in t:\n",
    "        avg_loss = 0.\n",
    "        for batch, _ in tqdm(dataloader, leave=False):\n",
    "            batch = batch.to(device)\n",
    "            model.zero_grad()\n",
    "            batch = flatten(batch)\n",
    "            reconstruction = model(batch)\n",
    "            loss = criterion(reconstruction, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "        avg_loss /= len(dataloader)\n",
    "        t.set_description(f\"Epoch {e}: loss = {avg_loss:.3f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671b80c",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Pourquoi privilégie-t-on dans ce cas précis la fonction de perte $ L_1 $ (*mean absolute error*) plutôt que de la fonction de perte $ L_2 $ (*mean squared error*) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5df4e",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "L’erreur quadratique moyenne « écrase » les petites valeurs de l’erreur (inférieures à 1) à cause du carré. Dans notre cas, les valeurs des pixels sont entre 0 et 1. Pour avoir un gradient plus élevé même pour des erreurs faibles, on utilise donc plutôt l’erreur absolue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15cfe2",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "En utilisant la fonction `train()` définie ci-dessus, réaliser l’apprentissage de l’autoencodeur défini précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86a83e",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7343bf9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "train(autoencoder, mnist, epochs=10, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49628287",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Pour quelques images du jeu de données, visualiser l’image originale et sa reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f2682",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fe874",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "data, _ = mnist[444]\n",
    "plt.imshow(data.numpy()[0]) and plt.show()\n",
    "with torch.no_grad():\n",
    "    reconst = autoencoder(data.reshape(1, -1)).numpy()[0].reshape(28, 28)\n",
    "plt.imshow(reconst) and plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48631b6",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Choisir deux images du jeu de données **de classes différentes**. Calculer leurs codes $ z_1 $ et $ z_2 $ à l’aide de l’encodeur. Afficher la reconstruction du code moyen $ 0,5 z_1 + 0,5 z_2 $. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c8ee4",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6656c4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "c1 = encoder(flatten(mnist[444][0]))\n",
    "c2 = encoder(flatten(mnist[3][0]))\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "alphas = np.linspace(0, 1, 10)\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    fig.add_subplot(1, len(alphas), idx+1)\n",
    "    c = alpha * c1 + (1 - alpha) * c2\n",
    "    with torch.no_grad():\n",
    "        img = decoder(c)\n",
    "        img = img.numpy()[0].reshape(28, 28)\n",
    "        plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e105e2b",
   "metadata": {},
   "source": [
    "La reconstruction du code interpolé n’est pas particulièrement réaliste et correspond à un mélange flou des deux images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51500e74",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Afficher les résultats des reconstructions de l’interpolation entre $ z_1 $ et $ z_2 $ pour différentes valeurs du facteur d’interpolation $ \\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4518e",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1916f32",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "   c1 = encoder(flatten(mnist[6][0]))\n",
    "   c2 = encoder(flatten(mnist[8][0]))\n",
    "   fig = plt.figure(figsize=(12, 6))\n",
    "   alphas = np.linspace(0, 1, 10)\n",
    "   for idx, alpha in enumerate(alphas):\n",
    "       fig.add_subplot(1, len(alphas), idx+1)\n",
    "       c = alpha * c1 + (1 - alpha) * c2\n",
    "       with torch.no_grad():\n",
    "           img = decoder(c)\n",
    "           img = img.numpy()[0].reshape(28, 28)\n",
    "           plt.imshow(img)\n",
    "   plt.show()\n",
    "\n",
    "On remarque que l'interpolation donne des résultats peu plausibles. Lorsque l'on s'éloigne des petites valeurs de :math:`\\alpha`. L'espace latent de l'auto-encodeur n'est pas aussi régulier qu'espéré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756dce9",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Construire un nouveau jeu de données contenant:\n",
    "\n",
    "- une matrice `codes`, qui contient pour chaque ligne $ i $ le code $ z_i $ obtenu en encodant l’image $ i $ du jeu de données `mnist`,  \n",
    "- une liste `labels` dont l’élément $ i $ représente l’étiquette de l’image $ i $ du jeu de données `mnist`.  \n",
    "\n",
    "\n",
    "Appliquer une réduction de dimension non-linéaire par l’algorithme [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) sur cette matrice de codes de sorte à pouvoir afficher un nuage de points dans le plan. On prendra soin de colorer chaque point en fonction de son étiquette.\n",
    "\n",
    "Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c862a2c",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "Sans aucune supervision, l’espace latent de l’auto-encodeur permet de relativement bien séparer les différentes classes d’objets avec une dimensionalité nettement inférieure à celle des données d’origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1815c5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "codes, labels = [], []\n",
    "for data, label in tqdm(mnist):\n",
    "    with torch.no_grad():\n",
    "        code = encoder(flatten(data))[0]\n",
    "    codes.append(code.numpy())\n",
    "    labels.append(label)\n",
    "\n",
    "codes = np.array(codes)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dca00",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(n_components=2)\n",
    "code_projections = proj.fit_transform(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e296e8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "scatter = plt.scatter(code_projections[:, 0], code_projections[:, 1], c=labels, cmap=plt.cm.Set1)\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                      loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e1bd1",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "*Approfondissement* (ne faites cette question qu’à la fin si vous avez encore du temps)\n",
    "\n",
    "Réaliser la même visualisation par t-SNE en utilisant non pas le code intermédiaire mais directement l’image aplatie. Que constatez-vous? Pourquoi ne peut-on pas utiliser cette approche dans le cas général?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971820d5",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "Le t-SNE appliqué directement sur l’image sépare encore mieux les données. On peut supposer que la réduction de dimension dans l’auto-encodeur nous a fait perdre un peu d’information. Dans le cas général, ce n’est pas possible d’appliquer t-SNE sur les données brutes : la dimensionalité des données est bien trop élevée. Cela ne fonctionne ici que parce que les images sont petites ($ 28 \\times 28 $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e6af8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "codes, labels = [], []\n",
    "for data, label in tqdm(mnist):\n",
    "    codes.append(data.reshape(-1).numpy())\n",
    "    labels.append(label)\n",
    "\n",
    "codes = np.array(codes)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f0f12",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(n_components=2)\n",
    "code_projections = proj.fit_transform(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144ef7a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "scatter = plt.scatter(code_projections[:, 0], code_projections[:, 1], c=labels, cmap=plt.cm.Set1)\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                      loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d79c4",
   "metadata": {},
   "source": [
    "## Exercice 3 : chaîne de Markov pour la génération de texte\n",
    "\n",
    "L’objectif de cet exercice est d’implémenter en Python pur une chaîne de\n",
    "Markov simple. Rappelons qu’une chaîne de Markov représente une séquence\n",
    "$ (X_t) $ où l’on s’intéresse à modéliser les probabilités de\n",
    "transitions qui permettent de passer d’un état $ i $ à un état\n",
    "$ j $, c’est-à-dire à la matrice:\n",
    "\n",
    "$$\n",
    "p_{i,j} = P(X_{t+1} = j | X_t = i)\n",
    "$$\n",
    "\n",
    "Pour cet exercice, nous allons utiliser un corpus textuel correspondant\n",
    "à la traduction française du roman *Frankenstein ou le Prométhée\n",
    "moderne* de l’écrivaine [Mary\n",
    "Shelley](https://fr.wikipedia.org/wiki/Mary_Shelley) (1797-1851).\n",
    "\n",
    "La modélisation du problème est la suivante:\n",
    "\n",
    "- un mot représente un état $ i $ de la chaîne,  \n",
    "- on considère la fréquence des\n",
    "  [bi-grammes](https://fr.wikipedia.org/wiki/N-gramme) de mots comme\n",
    "  probabilités de transition.  \n",
    "\n",
    "\n",
    "Pour être plus précis, on considérera que l’ensemble $ E $ des états\n",
    "possibles correspond à un dictionnaire (fini) de mots. Pour chaque\n",
    "bi-gramme, c’est-à-dire chaque paire de mots\n",
    "$ (X_t = i, X_{t+1} = j) $, nous allons calculer son nombre\n",
    "d’occurrences $ N_{i,j} $.\n",
    "\n",
    "Pour un mot donné $ i $, sa probabilité de transition vers le mot\n",
    "suivant $ j $ correspond à la probabilité d’occurrence du bigramme\n",
    "$ (i,j) $, c’est-à-dire :\n",
    "\n",
    "$$\n",
    "p_{i,j} = \\frac{N_{i,j}}{\\sum_k N_{i,k}}\n",
    "$$\n",
    "\n",
    "Observons que le cas des chaînes de Markov d’ordre 2 et supérieur\n",
    "correspond à s’intéresser aux n-grammes pour $ n>2 $.\n",
    "\n",
    "**Note**: les mots du dictionnaire $ E $ sont à prendre au sens\n",
    "large. En particulier, on considérera que les symboles de ponctuation\n",
    "(?, !, …, etc.) constituent des mots à part entière dans le\n",
    "dictionnaire.\n",
    "\n",
    "Afin de débuter cet exercice, commençons par examiner le corpus et créer\n",
    "le dictionnaire d’états $ E $. Pour ce faire, nous transformer la\n",
    "chaîne de caractères qui constitue l’intégralité du texte en une\n",
    "séquence de mots. Cette opération s’appelle la *tokenization* (ou\n",
    "segmentation). Elle s’appuie sur des règles de grammaire propres à\n",
    "chaque langage.\n",
    "\n",
    "**Note**: Le fonctionnement précis de ce processus est hors-sujet pour\n",
    "ce cours, se référer au module de traitement automatique du langage de\n",
    "RCP217 pour plus de détails.\n",
    "\n",
    "Nous allons utiliser la bibliothèque de traitement du langage `nltk`\n",
    "pour réaliser la *tokenization*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c692b9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a59c07",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# Téléchargement du fichier\n",
    "r = requests.get(\"https://cedric.cnam.fr/vertigo/Cours/RCP211/data/frankenstein.txt\")\n",
    "with open(\"frankenstein.txt\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "with open(\"frankenstein.txt\") as f:\n",
    "    corpus = word_tokenize(f.read(), language='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea940e",
   "metadata": {},
   "source": [
    "Nous pouvons désormais afficher le début du corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca38a1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "\" \".join(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a90daa",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Combien de mots uniques ce corpus contient-il ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b129d41",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f63e3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "words = np.unique(corpus)\n",
    "word_index = {word: idx for idx, word in enumerate(words)}\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ed9c8",
   "metadata": {},
   "source": [
    "Afin de créer une chaîne de Markov d’ordre 1 sur ces données, il nous\n",
    "faut produire la liste de tous les bi-grammes contenus dans le texte.\n",
    "Nous pouvons les générer avec la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30817459",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def make_pairs(corpus):\n",
    "    for i in range(len(corpus) - 1):\n",
    "        yield (corpus[i], corpus[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607b665",
   "metadata": {},
   "source": [
    "Nous allons utiliser l’algorithme suivant pour construire la matrice de\n",
    "transition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b884f04",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Créer un dictionnaire d vide\n",
    "Pour chaque paire (i, j) du corpus:\n",
    "      Si d n'a pas d'entrée pour i:\n",
    "          d[i] = dictionnaire vide\n",
    "      Si d[i] n'a pas d'entrée pour j:\n",
    "          d[i][j] = 1\n",
    "      Sinon:\n",
    "          d[i][j] += 1\n",
    "Renvoyer d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313bedbd",
   "metadata": {},
   "source": [
    "Remarquons que cet algorithme ne construit pas exactement la matrice de\n",
    "transition. Elle produit un double dictionnaire (c’est-à-dire un\n",
    "dictionnaire dont les éléments sont également des dictionnaires) tel que\n",
    "`d[mot1][mot2]` renvoie le nombre d’occurrences du bi-gramme (mot1,\n",
    "mot2) dans le corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766612fc",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Écrire une fonction `build_transition_matrix` qui implémente cet algorithme. Les paires sont calculées à l’aide de la fonction `make_pairs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10e887",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953996d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_transition_matrix(corpus):\n",
    "    \"\"\"\n",
    "        Construit un double dictionnaire d tel que:\n",
    "            d[mot1][mot2] = nombre d'occurrences du bi-gramme (mot1, mot2) dans le corpus\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        corpus (str list): une liste de mots\n",
    "\n",
    "        Renvoie\n",
    "        -------\n",
    "        dict: double dictionnaire des nombres d'occurrences des bi-grammes\n",
    "    \"\"\"\n",
    "    pairs = make_pairs(corpus)\n",
    "    transition_matrix = {}\n",
    "\n",
    "    for word1, word2 in pairs:\n",
    "        if word1 not in transition_matrix.keys():\n",
    "            transition_matrix[word1] = {}\n",
    "        if word2 not in transition_matrix[word1].keys():\n",
    "            transition_matrix[word1][word2] = 1\n",
    "        else:\n",
    "            transition_matrix[word1][word2] += 1\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657593b",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Pourquoi utiliser un dictionnaire plutôt qu’une matrice (dense) de transition ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645403f6",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "La matrice dense de transition devrait contenir une entrée pour chaque bi-gramme, c’est-à-dire $ N\\times N $ entrées. Vu le nombre de mots unique du dictionnaire, cette matrice occuperait beaucoup de mémoire. L’utilisation du dictionnaire permet de ne stocker réellement que les bi-grammes qui existent (et donc d’ignorer tous les 0 de la matrice de transition).\n",
    "\n",
    "Nous pouvons désormais calculer la “matrice de transition”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616be1d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "transition_matrix = build_transition_matrix(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c433eaa",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Afficher les 10 bi-grammes les plus fréquents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6715eb",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b05f2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigrams = defaultdict(lambda: 0)\n",
    "for w1, d in transition_matrix.items():\n",
    "    for w2, c in d.items():\n",
    "        bigrams[w1, w2] = c\n",
    "\n",
    "sorted(bigrams.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cceef",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Afficher les probabilités de transition pour tous les bi-grammes commençant par “une”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7a627",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc0447",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "transition_matrix[\"une\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ff1b8",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Écrire une fonction `sample_next_word(word, transition_matrix)` qui, à partir d’un mot `word` et de la matrice de transition tire au hasard le mot suivant en respectant les probabilités de transition $ p_{i,j} $.\n",
    "\n",
    "**Indice**: on pourra utiliser à bon escient la fonction `np.random.choice` de [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058c96e",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca41070",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sample_next_word(word, transition_matrix):\n",
    "    \"\"\"\n",
    "       Renvoie un mot au hasard complétant la séquence en suivant les probabilités de transition\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "        - word (str): un mot (première moitié du bi-gramme)\n",
    "        - transition_matrix (dict): double dictionnaire du nombre d'occurrences\n",
    "\n",
    "       Renvoie\n",
    "       -------\n",
    "         - str: un mot complétant le bi-gramme\n",
    "    \"\"\"\n",
    "    # Dictionnaire des transitions pour \"word\"\n",
    "    transitions = transition_matrix[word]\n",
    "    # Bi-grammes possibles\n",
    "    candidates = np.array(list(transitions.keys()))\n",
    "    # Nombre d'occurrence des bi-grammes commençant par \"word\"\n",
    "    weights = np.array(list(transitions.values()))\n",
    "    # Tirage aléatoire\n",
    "    next_word = np.random.choice(candidates,\n",
    "                                 p=weights/sum(weights))\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760544e",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Utiliser cette fonction pour générer une phrase commençant par “Une”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469cb48",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b548ee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "w = \"Une\"\n",
    "sentence = [w]\n",
    "while w != \".\":\n",
    "    w = sample_next_word(w, transition_matrix)\n",
    "    sentence.append(w)\n",
    "print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbddc9",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "*Approfondissement* (ne faites cette question que si vous avez le temps)\n",
    "\n",
    "Réaliser le même travail mais dans le cas de la chaîne de Markov d’ordre 2 (raisonner sur des triplets plutôt que des pairs).\n",
    "\n",
    "**Indice**: on pourra utiliser [defaultdict](https://docs.python.org/fr/3/library/collections.html#collections.defaultdict) pour écrire plus simplement la fonction de construction de la matrice de transition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb0128",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e0464",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def make_triplets(corpus):\n",
    "    for i in range(len(corpus) - 2):\n",
    "        yield (corpus[i], corpus[i+1], corpus[i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe20c5d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_transition_matrix_triplets(triplets):\n",
    "    triplets = make_triplets(corpus)\n",
    "    transition_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for word1, word2, word3 in triplets:\n",
    "        transition_matrix[word1, word2][word3] += 1\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cd940",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "triplet_transition_matrix = build_transition_matrix_triplets(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b1a58",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sample_next_word_triplets(word1, word2, transition_matrix):\n",
    "    # Dictionnaire des transitions pour \"word1\", \"word2\"\n",
    "    transitions = transition_matrix[word1, word2]\n",
    "    # Bi-grammes possibles\n",
    "    candidates = np.array(list(transitions.keys()))\n",
    "    # Nombre d'occurrence des tri-grammes commençant par \"word1\", \"word2\"\n",
    "    weights = np.array(list(transitions.values()))\n",
    "    # Tirage aléatoire\n",
    "    next_word = np.random.choice(candidates,\n",
    "                                 p=weights/sum(weights))\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526cf39",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "w1, w2 = \"un\", \"homme\"\n",
    "sentence = [w1, w2]\n",
    "\n",
    "while w2 != \".\":\n",
    "    w = sample_next_word_triplets(w1, w2, triplet_transition_matrix)\n",
    "    w1 = w2\n",
    "    w2 = w\n",
    "    sentence.append(w2)\n",
    "\n",
    "print(\" \".join(sentence))"
   ]
  }
 ],
 "metadata": {
  "date": 1649691302.1261613,
  "filename": "TP6-IntroGeneration.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python"
  },
  "title": "Travaux pratiques : introduction aux modèles génératifs"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}