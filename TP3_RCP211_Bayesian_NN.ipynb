{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TP3_RCP211_Bayesian_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgNFac420Nh_"
      },
      "source": [
        "## I : Variational Layer (recap)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfoBwex9f1I8"
      },
      "source": [
        "#Utils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "import torch.distributions as dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2eNammRvajL"
      },
      "source": [
        "**Optimization problem**  \r\n",
        "We define an approximating variational distribution $q_{\\pmb{\\theta}}(\\pmb{w})$ parametrized by $\\pmb{\\theta}$ and minimize its Kullback-Leibler (KL) divergence with the unknown true posterior $p(\\pmb{w} \\vert \\mathcal{D})$. This is equivalent to maximizing the **evidence lower bound (ELBO)** w.r.t to $q_{\\pmb{\\theta}}(\\pmb{w})$:\r\n",
        "\r\n",
        "$$ arg \\max_{\\pmb{\\theta}}~ \\mathbb{E}_{q_{\\pmb{\\theta}}(\\pmb{w})} \\big [\\underbrace{\\log p(\\mathcal{D} \\vert \\pmb{w})}_{likelihood} \\big ] - \\underbrace{\\textrm{KL}(q_{\\pmb{\\theta}}(\\pmb{w})\\vert\\vert p(\\pmb{w}))}_{regularization} $$\r\n",
        "\r\n",
        "**Monte Carlo estimator**  \r\n",
        "Maximizing the ELBO is the same as minimizing its negative logarithm. The expectetion is obtained through Monte Carlo sampling:\r\n",
        "\r\n",
        "$$ \\mathcal{L}_{\\textrm{VI}}(\\pmb{\\theta}; \\mathcal{D}) = - \\sum_{n=1}^N \\Big ( \\log p(y_n \\vert \\pmb{x}_n, \\pmb{w}_s) + \\frac{1}{N} \\big ( \\log q_{\\pmb{\\theta}}(\\pmb{w}_s) - \\log p(\\pmb{w}_s) \\big ) \\Big )$$\r\n",
        "where $\\pmb{w}_s \\sim q_{\\pmb{\\theta}}$ is a sample from the variational distribution. <br/><br/>\r\n",
        "\r\n",
        "**Reparametrization trick**  \r\n",
        "The stochastic nature of the layer is obtained through a parametrization with a multivariate gaussian random variable:\r\n",
        "$$ \\pmb{W}_{l,s} = \\pmb{\\mu}_{l}+ \\pmb{\\Sigma}_{l}\\odot\\varepsilon $$\r\n",
        "where $\\varepsilon \\sim \\mathcal{N}(0,1)$.\r\n",
        "<br/><br/>  \r\n",
        "\r\n",
        "\r\n",
        "Let's first re-implement the variational layer as we defined it in the last PS. Remind that we defined our Logistic regression model \r\n",
        "as $f(x) = \\sigma(w^T x + b)$ where $\\sigma(t)= \\frac{1}{1+\\exp(t)}$ is the sigmoid function. As such,\r\n",
        " we need to place Gaussian distributions on parameters $w$ and $b$.  \r\n",
        "\r\n",
        "**Reminder** Variance can not be negative. To avoid numerical issues, we will use $\\pmb{\\rho}$. Std can be retrieve with the following formula:\r\n",
        "$\\pmb{\\Sigma} = \\log(1 + e^{\\pmb{\\rho}})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX10v7G3ND6G"
      },
      "source": [
        "class KL:\n",
        "    accumulated_kl_div = 0\n",
        "\n",
        "def elbo(input, target, model):\n",
        "  negative_log_likelihood = -dist.Binomial(logits=input).log_prob(target).sum()\n",
        "  return negative_log_likelihood + model.accumulated_kl_div"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb6ofPETDZOb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "ea0c003d-70bf-4bd4-a1c5-4335ca5345f8"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a variational layer from scratch\r\n",
        "\r\n",
        "class LinearVariational(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Mean field approximation of nn.Linear\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, input_size, output_size, parent):\r\n",
        "      super().__init__()\r\n",
        "      self.parent =  parent\r\n",
        "      \r\n",
        "      if getattr(parent, 'accumulated_kl_div', None) is None:\r\n",
        "          parent.accumulated_kl_div = 0\r\n",
        "      \r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Initialize the variational parameters for weight and bias\r\n",
        "      # with nn.Parameter.\r\n",
        "      # Mean should be initialised to zeros and rho to ones\r\n",
        "      self.w_mu = None\r\n",
        "      self.w_rho = None\r\n",
        "      self.b_mu = None\r\n",
        "      self.b_rho = None\r\n",
        "          \r\n",
        "    def sampling(self, mu, rho):\r\n",
        "        \"Sample weights using the reparametrization trick\"\r\n",
        "        # ============ YOUR CODE HERE ============\r\n",
        "        # Given parameter mu and rho, return sampling using \r\n",
        "        # the reparametrization trick.\r\n",
        "        # NB: you may look for torch.randn_like...\r\n",
        "        sigma = # torch.log(..)\r\n",
        "        eps = # torch.randn_like(..)\r\n",
        "        w = mu + sigma*eps\r\n",
        "        return w\r\n",
        "    \r\n",
        "    def kl_divergence(self, z, mu_theta, rho_theta, prior_sd=1):\r\n",
        "    \"Computing the KL-divergence term for these weight's parameters\"\r\n",
        "    log_prior = dist.Normal(0, prior_sd).log_prob(z) \r\n",
        "    log_p_q = dist.Normal(mu_theta, torch.log(1 + torch.exp(rho_theta))).log_prob(z)\r\n",
        "    return (log_p_q - log_prior).sum() / X.shape[0]\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"Usual forward function for pytorch layer\"\r\n",
        "        # ============ YOUR CODE HERE ============\r\n",
        "        # Sample parameters w and b using self.sampling\r\n",
        "        w = # call self.sampling\r\n",
        "        b = # call self.sampling\r\n",
        "\r\n",
        "        # Then, perform a forward pass using those sampled parameters\r\n",
        "        out = # use torch.matmul for the matrix multiplication and add the bias\r\n",
        "        \r\n",
        "        # Compute KL-div loss for training\r\n",
        "        self.parent.accumulated_kl_div += self.kl_divergence(w, self.w_mu, self.w_rho)\r\n",
        "        self.parent.accumulated_kl_div += self.kl_divergence(b, self.b_mu, self.b_rho)\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-833b9c90b97b>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    sigma = # torch.log(..)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZV6pC6v7Bqh"
      },
      "source": [
        "## II. Variational Inference with Bayesian Neural Network\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqscdtHyT97"
      },
      "source": [
        "Moving on to a non-linear dataset, we will leverage our variational implementation to a Multi-Layer Perceptron (MLP). Finally, we will also review one last approximate inference method which has the particularity to be very easy to implement: Monte-Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g0QmYJ7WJ8p"
      },
      "source": [
        "### II.0 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVUva--mk2jk",
        "cellView": "form"
      },
      "source": [
        "#@title Hyperparameters for model and approximate inference { form-width: \"30%\" }\r\n",
        "\r\n",
        "NOISE_MOON = 0.05 #@param\r\n",
        "WEIGHT_DECAY = 5e-2 #@param\r\n",
        "NB_SAMPLES = 100 #@param\r\n",
        "TEXT_LOCATION = (-1.5, -1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjIhWLPrkVdp"
      },
      "source": [
        "# Load two moons dataset\r\n",
        "X, y = make_moons (n_samples=1000, noise=NOISE_MOON)\r\n",
        "X, y = torch.from_numpy(X), torch.from_numpy(y)\r\n",
        "X, y = X.type(torch.float), y.type(torch.float)\r\n",
        "torch_train_dataset = data.TensorDataset(X,y) # create your datset\r\n",
        "train_dataloader = data.DataLoader(torch_train_dataset, batch_size=len(torch_train_dataset))\r\n",
        "N_DIM = X.shape[1]\r\n",
        "\r\n",
        "# Visualize dataset\r\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired_r', edgecolors='k')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0rMZBLxbHY9"
      },
      "source": [
        "### II.1 Variational Inference with Bayesian Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV8BP1t1yuu1"
      },
      "source": [
        "Such as for Logistic Regression, we will use `LinearVariational` layer to define a MLP with 1 hidden layer.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCOP9aDQo7zh"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a Variational MLP\r\n",
        "\r\n",
        "class VariationalMLP(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "      super().__init__()\r\n",
        "      self.kl_loss = KL\r\n",
        "\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Define a variational MLP with 1 hidden layer and ReLU activation\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def accumulated_kl_div(self):\r\n",
        "      return self.kl_loss.accumulated_kl_div\r\n",
        "    \r\n",
        "    def reset_kl_div(self):\r\n",
        "      self.kl_loss.accumulated_kl_div = 0\r\n",
        "            \r\n",
        "    def forward(self, x):\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Don't forget to apply the sigmoid function when returning the output\r\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7NXnFs0AmB"
      },
      "source": [
        "We can now train our variational model as any other network in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar5ZX1SfpdgV"
      },
      "source": [
        "var_net = VariationalMLP(input_size=X.shape[1], hidden_size=50)\r\n",
        "var_net.train()\r\n",
        "optimizer = torch.optim.Adam(var_net.parameters(), lr=0.1, weight_decay=WEIGHT_DECAY)\r\n",
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "\r\n",
        "for epoch in range(1000):  # loop over the dataset multiple times\r\n",
        "    # zero the parameter gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    var_net.reset_kl_div()\r\n",
        "\r\n",
        "    # forward + backward + optimize\r\n",
        "    output = var_net(X).squeeze()\r\n",
        "    loss = elbo(output, y, var_net)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # For plotting and showing learning process at each epoch\r\n",
        "    if (epoch+1)%50==0:\r\n",
        "      # Computing prediction for visualization purpose\r\n",
        "      preds = torch.zeros(NB_SAMPLES, X.shape[0], 1)\r\n",
        "      for i in range(NB_SAMPLES):\r\n",
        "          preds[i] = var_net(X)\r\n",
        "      pred = preds.mean(0).squeeze()\r\n",
        "      accuracy = ((pred>=0.5) == y).float().mean()\r\n",
        "\r\n",
        "      plot_decision_boundary(var_net, X, y, epoch, accuracy, model_type='vi', tloc=TEXT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_scDA7bC1c"
      },
      "source": [
        "### II.2 Monte Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8J9UNT41TJ2"
      },
      "source": [
        "Training a neural network with randomly dropping some activations, such as with dropout layers, can actually be seen as an **approximate variational inference method**!\r\n",
        "\r\n",
        "[Gal and Ghahramani, 2016] showed this can be fullfilled for:\r\n",
        "- $p(\\pmb{w}) = \\prod_l p(\\pmb{W}_l) = \\prod_l \\mathcal{MN}(\\pmb{W}_l; 0, I/ l_i^2, I)$ $\\Rightarrow$ Multivariate Gaussian distribution factorized over layers\r\n",
        "- $q(\\pmb{w}) = \\prod_l q(\\pmb{W}_l) = \\prod_l \\textrm{diag}(\\varepsilon_l)\\odot\\pmb{M}_l $ with $\\varepsilon_l \\sim \\textrm{Ber}(1-p_l)$.\r\n",
        "\r\n",
        "We will now implement a MLP with dropout layers and perform Monte-Carlo sampling to obtain the predictive distribution $p(\\mathbf{y} \\vert \\pmb{x}^*, \\pmb{w})$ for a new sample $\\pmb{x}^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxGmkjd0kVgZ"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a MLP with dropout (p=0.2)\r\n",
        "# Code MLP with 1 hidden layer and a dropout layer. Be careful, the dropout \r\n",
        "# layer should be also activated during test time.\r\n",
        "# (Hint: we may want to look out at F.dropout())\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "    \"\"\" Pytorch MLP for binary classification model with an added dropout layer\"\"\"\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Don't forget to apply the sigmoid function when returning the output\r\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMlasuRN08Yk"
      },
      "source": [
        "We train our model as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqSZJDWIlf5e"
      },
      "source": [
        "net = MLP(input_size=X.shape[1], hidden_size=50)\r\n",
        "net.train()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\r\n",
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "\r\n",
        "for epoch in range(500):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    # zero the parameter gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # forward + backward + optimize\r\n",
        "    output = net(X).squeeze()\r\n",
        "    loss = criterion(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # For plotting and showing learning process at each epoch, uncomment and indent line below\r\n",
        "    if (epoch+1)%50==0:\r\n",
        "      plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='classic')\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RyglMRa4d5"
      },
      "source": [
        "Now let's look at the results given by MC Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRh6BQDcazFx"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='mcdropout')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05OBuwko5EvS"
      },
      "source": [
        "**[Question 2.1]: Again, analyze the results showed on plot. What is the benefit of MC Dropout variational inference over Bayesian Logistic Regression with variational inference?**"
      ]
    }
  ]
}