{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42007b52",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpvae-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ee86e",
   "metadata": {},
   "source": [
    "# Travaux pratiques : auto-encodeurs variationnels\n",
    "\n",
    "L’objectif de cette séance de travaux pratiques est d’illustrer la\n",
    "construction d’un espace latent en utilisant un auto-encodeur classique,\n",
    "puis un auto-encodeur variationnel. (cf. [Kingma et\n",
    "Welling](https://arxiv.org/pdf/1312.6114.pdf) ).\n",
    "\n",
    "Nous utiliserons ensuite le VAE pour générer des données synthétiques\n",
    "mais plausibles vis à vis de la distribution des données\n",
    "d’apprentissage.\n",
    "\n",
    "Le TP est dimensionné de sorte à pouvoir être terminé sans accélérateur\n",
    "graphique pour les calculs en utilisant la base de données\n",
    "Fashion-MNIST. Si vous disposez d’un GPU (par exemple, parce que vous\n",
    "travaillez sur Google Colab), vous pouvez changer la valeur de la\n",
    "variable `use_gpu` à `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36682b12",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "use_gpu = False\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Exécution sur {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38529ec",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Imports des bibliothèques utiles\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4722b59",
   "metadata": {},
   "source": [
    "## Préambule\n",
    "\n",
    "Pour commencer, nous allons charger en mémoire les données de\n",
    "Fashion-MNIST et en visualiser quelques unes. Ces images sont similaires\n",
    "en format aux données de MNIST : 28x28 pixels en niveaux de gris.\n",
    "\n",
    "Ce jeu de données est préintégré dans la bibliothèque `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667d4e4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=ToTensor())\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b49ec3",
   "metadata": {},
   "source": [
    "Nous pouvons visualiser quelques unes de ces images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed27c59",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, (image, label) in enumerate(train_dataset):\n",
    "    fig.add_subplot(1, n_images+1, i+1)\n",
    "    plt.imshow(ToPILImage()(image), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i >= n_images:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe296ca",
   "metadata": {},
   "source": [
    "## Auto-encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a600019",
   "metadata": {},
   "source": [
    "### Implémentation\n",
    "\n",
    "Notre premier modèle sera un auto-encodeur convolutif doté de\n",
    "l’architecture ci-dessous.\n",
    "\n",
    "Pour l’encodeur :\n",
    "\n",
    "- une [couche de convolution](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) (`kernel_size=4`, `in_channels=1`, `out_channels=32`, `stride=2`, `padding=1`, activation ReLU)  \n",
    "- une [couche de convolution](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) (`kernel_size=4`, `in_channels=32`, `out_channels=64`, `stride=2`, `padding=1`, activation ReLU)  \n",
    "- une [couche linéaire](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) (`in_features=64*7*7`, `out_features=latent_dimension`)  \n",
    "\n",
    "\n",
    "Pour le décodeur:\n",
    "\n",
    "- une [couche linéaire](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) (`in_features=latent_dimension`, `out_features=64*7*7`, activation ReLU)  \n",
    "- une couche de [convolution transposée](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) (`kernel size=4`, `in_channels=64`, `out_channels=32`, `stride=2`, `padding=1`, activation ReLU)  \n",
    "- une couche de [convolution transposée](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) (`kernel size=4`, `in_channels=32`, `out_channels=1`, `stride=2`, `padding=1`, activation sigmoide)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e494d",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Les filtres convolutifs sont choisis de taille 4x4 afin d’éviter des [problèmes d’aliasing](https://distill.pub/2016/deconv-checkerboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fdf4c",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compléter l’implémentation ci-dessous de l’auto-encodeur dont l’architecture vient d’être décrite. Cette implémentation utilise l’interface [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) dont la documentation peut vous être utile.\n",
    "\n",
    "En plus de la reconstruction par l’auto-encodeur, on souhaite que la méthode `forward()` renvoie également le code intermédiaire `z` (un vecteur de longueur `latent_dimension`) obtenu après le passage dans le décodeur.\n",
    "\n",
    "**Indice**: l’utilisation de la méthode `.view()` ou de la couche `nn.Flatten()` peut être utile pour ré-arranger les tenseurs avant ou après les couches linéaires. Par exemple, `x.view(-1, 64, 7, 7)` permet de transformer un tenseur de dimensions `(batch, 3136)` en un tenseur de dimensions `(batch, 64, 7, 7)`…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b962fc",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee82e0c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Flatten(),\n",
    "                                     nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "                                     )\n",
    "        self.decoder_linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###### Votre code ici ########\n",
    "        z = self.encoder(x)\n",
    "        hat_x = F.relu(self.decoder_linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.decoder(hat_x)\n",
    "        return hat_x, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0ad77",
   "metadata": {},
   "source": [
    "### Entraînement\n",
    "\n",
    "Une fois le modèle implémenté, nous pouvons utiliser la fonction\n",
    "`train` ci-dessous pour réaliser l’apprentissage. L’optimisation se\n",
    "fait selon le critère choisi dans la variable `criterion` (par défaut,\n",
    "il s’agit de l’erreur quadratique moyenne comme critère de\n",
    "reconstruction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3909b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def train(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    # Création du DataLoader pour charger les données\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    # Choix de la fonction de coût\n",
    "    criterion = nn.MSELoss()\n",
    "    # Passe le modèle en mode \"apprentissage\"\n",
    "    net = net.to(device)\n",
    "    net = net.train()\n",
    "\n",
    "    train_loss_avg = []\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        # Parcours du dataset pour une epoch\n",
    "        for images, _ in tqdm(train_dataloader):\n",
    "            # les labels sont ignorés pour l'apprentissage de l'auto-encodeur\n",
    "\n",
    "            images = images.to(device)\n",
    "            # Calcul de la reconstruction\n",
    "            reconstructions, _ = net(images)\n",
    "            # Calcul del'erreur\n",
    "            loss = criterion(reconstructions, images)\n",
    "\n",
    "            # Rétropropagation du gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Descente de gradient (une itération)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c1834",
   "metadata": {},
   "source": [
    "Nous pouvons créer un modèle en spécifiant la dimension de son espace\n",
    "latent (par exemple, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b29113",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "latent_dimension = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af665d8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "net = AutoEncoder(latent_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48095e3c",
   "metadata": {},
   "source": [
    "Puis démarrer son entraînement (sur CPU, cette opération peut prendre\n",
    "jusqu’à une dizaine de minutes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf240c9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "net = train(net, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee0152",
   "metadata": {},
   "source": [
    "### Visualisation des reconstructions\n",
    "\n",
    "Une fois l’apprentissage terminé, nous pouvons visualiser quelques\n",
    "reconstructions obtenues grâce à l’auto-encodeur. Cela permet de jauger\n",
    "qualitativement des performances du modèle en reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e66abe",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "La compréhension fine des fonctions `make_grid`, `show_grid` et `visualize_reconstructions` n’est pas indispensable à la poursuite du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126bb93",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "net = net.eval()\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "def show_grid(grid):\n",
    "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_reconstructions(net, images, device=device):\n",
    "    # Mode inférence\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        reconstructions = net(images)[0]\n",
    "        image_grid = make_grid(reconstructions[1:50], 10, 5).cpu()\n",
    "        return image_grid\n",
    "\n",
    "images, _ = iter(test_dataloader).next()\n",
    "\n",
    "# Images de test\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Images du jeu de test\")\n",
    "show_grid(make_grid(images[1:50],10,5))\n",
    "\n",
    "# Reconstruction et visualisation des images reconstruites\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Reconstruction par l'auto-encodeur\")\n",
    "show_grid(visualize_reconstructions(net, images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9bb9f",
   "metadata": {},
   "source": [
    "### Débruitage\n",
    "\n",
    "Une capacité intéressante des auto-encodeurs est leur capacité à\n",
    "apprendre des filtres robustes au bruit. En particulier, en bruitant\n",
    "légèrement une observation, on retrouve généralement la reconstruction\n",
    "moyenne *non-bruitée*. Cette propriété de débruitage est\n",
    "particulièrement intéressante pour l’amélioration de la qualité des\n",
    "signaux (images, sons, etc.).\n",
    "\n",
    "Observons la capacité de débruitage de notre auto-encodeur sur un\n",
    "échantillon d’images de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2376dd1",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compléter le code ci-dessous pour ajouter un bruit blanc uniforme aux images de test contenues dans le tenseur `images`. Pensez à ajuster l’amplitude du bruit et à limiter les valeurs des pixels de sortie à la plage autorisée $ [0,1] $ (la fonction `clamp` peut vous aider)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2a01a",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78004b18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Bruit blanc (uniforme) centré en 0\n",
    "noise = torch.rand_like(images) - 0.5\n",
    "# Ajout du bruit + troncature des valeurs en dehors de [0,1]\n",
    "noisy_images = torch.clamp(images + 0.5 * noise, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db1ea0",
   "metadata": {},
   "source": [
    "Nous pouvons alors visualiser la reconstruction des images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bf6d9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Images de test\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Images du jeu de test bruitées\")\n",
    "show_grid(make_grid(noisy_images[1:50],10,5))\n",
    "\n",
    "# Reconstruction et visualisation des images reconstruites\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Reconstruction par l'auto-encodeur\")\n",
    "show_grid(visualize_reconstructions(net, noisy_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90260bb8",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Comparer les reconstructions des images bruitées aux reconstructions obtenues sur les images de test originales. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72753b5f",
   "metadata": {},
   "source": [
    "### Correction\n",
    "\n",
    "Pour des bruits de faible amplitude, les images reconstruites sont moins bruitées. Autrement dit, l’auto-encodeur « débruite » en partie les images. En revanche, lorsque l’amplitude du bruit est forte, le signal utile de l’image disparaît et la sortie devient décorrélée de l’entrée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac006fa",
   "metadata": {},
   "source": [
    "## Auto-encodeurs variationnels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f6b09",
   "metadata": {},
   "source": [
    "### Implémentation\n",
    "\n",
    "Nous allons à présent implémenter un VAE convolutif qui hérite de la\n",
    "même structure que l’auto-encodeur que nous avons précédemment défini.\n",
    "Pour nous simplifier les choses par la suite, nous allons commencer par\n",
    "séparer le sous-réseau qui définit l’encodeur de celui qui définit le\n",
    "décodeur.\n",
    "\n",
    "Question: en reprenant ce qui a été fait plus haut pour l’auto-encodeur\n",
    "classique, compléter les implémentations ci-dessous de l’encodeur et du\n",
    "décodeur pour le VAE. On rappelle que, contrairement à l’auto-encodeur,\n",
    "la sortie de l’encodeur est double :\n",
    "\n",
    "- le vecteur `mu` qui contient la moyenne de la gaussienne dans\n",
    "  l’espace latent,  \n",
    "- le vecteur `sigma` qui contient les variances selon les différentes\n",
    "  directions de la gaussienne dans l’espace latent.  \n",
    "\n",
    "\n",
    "Ces valeurs seront les paramètres de la gaussienne associée à une\n",
    "observation $ x $. Ces deux vecteurs ont pour dimension la dimension\n",
    "de l’espace latent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846c920",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d3e9e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Flatten(),\n",
    "                                   )\n",
    "        self.linear1 = nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "        self.linear2 = nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x_mu = self.linear1(x)\n",
    "        x_logvar = self.linear2(x)\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7)\n",
    "        self.model = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, z):\n",
    "        hat_x = F.relu(self.linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.model(hat_x)\n",
    "        return hat_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f271a",
   "metadata": {},
   "source": [
    "Nous allons à présent combiner l’encodeur et le décodeur pour former\n",
    "l’auto-encodeur variationnel complet. Il y a néanmoins une petite\n",
    "subtilité car nous devons implémenter l’astuce de reparamétrisation.\n",
    "Celle-ci est implémenter dans la méthode `latent_sample`\n",
    "\n",
    "Lors d’un passage avant (`forward`), le schéma suivant doit se\n",
    "dérouler :\n",
    "\n",
    "1. L’encodeur prend `x` en entrée et produit la moyenne `mu` et la\n",
    "  variance `logvar` de la distribution. En pratique, on verra aussi\n",
    "  `x_recon` la reconstruction de `x`.  \n",
    "1. On tire un échantillon aléatoire `z` dans l’espace latent à l’aide\n",
    "  de la méthode `latent_sample`. L’échantillonnage est fait selon la\n",
    "  distribution gaussienne latente associée à $ x $ grâce à la\n",
    "  reparamétrisation. Lors de l’inférence, on ne réalisera pas\n",
    "  d’échantillonnage mais on se contentera d’utiliser la moyenne de la\n",
    "  gaussienne.  \n",
    "1. L’échantillon aléatoire `z` est passé dans le décodeur de sorte à\n",
    "  obtenir la reconstruction `x_recon`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8b8d9",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compléter l’implémentation ci-dessous de l’auto-encodeur variationnel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22279c5a",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcd650",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        z = self.latent_sample(latent_mu, latent_logvar)\n",
    "        hat_x = self.decoder(z)\n",
    "        return hat_x, latent_mu, latent_logvar\n",
    "\n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4e01d",
   "metadata": {},
   "source": [
    "Enfin, il reste à définir la fonction de coût du VAE. D’après le cours,\n",
    "on cherche à maximiser l’ELBO $ \\mathcal L $. Ici, on choisira de\n",
    "minimiser $ -\\mathcal L $ avec\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi ; \\boldsymbol x) =  \\underbrace{\\mathbb{E}_{q_\\phi(\\boldsymbol z | \\boldsymbol x)} \\left [ \\log p_\\theta(\\boldsymbol x | \\boldsymbol z) \\right ]}_{\\text{Espérance de la vraisemblance}} - \\underbrace{KL\\, \\left (q_\\phi(\\boldsymbol z | \\boldsymbol x) \\, || \\, p_\\theta(\\boldsymbol z)\\right)}_{\\text{écart au prior}}\n",
    "$$\n",
    "\n",
    "La fonction de coût pour une reconstruction sur une seule donnée\n",
    "$ \\boldsymbol x^{(i)} $ est approximée par:\n",
    "\n",
    "$$\n",
    "-\\mathcal{L}(\\theta,\\phi ; \\boldsymbol x^{(i)})  \\simeq - \\frac{1}{2} \\sum_j^d \\bigl ( 1 + \\log((\\sigma_j^{(i)})^2) - (\\mu_j^{(i)})^2 - (\\sigma_j^{(i)})^2  \\bigr) - \\log p_\\theta(\\boldsymbol x^{(i)} | \\boldsymbol z^{(i)})\n",
    "$$\n",
    "\n",
    "où $ d $ est la taille de l’espace latent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806143db",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Dans la plupart des cas, la vraisemblance est supposée gaussienne et la fonction de coût évaluant la reconstruction correspondera donc à l’erreur quadratique moyenne (`F.mse_loss()`). Dans notre cas, la distribution des valeurs des pixels de Fashion-MNIST est plutôt bimodale. Les images étant à valeurs entre 0 et 1, il est possible d’utiliser une entropie croisée binaire (`F.bce_loss()`) et c’est cette version qui donne les meilleurs résultats.\n",
    "\n",
    "Le *prior* $ p_\\theta(\\boldsymbol z) $ est supposé être donné par\n",
    "une loi normale centrée réduite. La divergence de Kullback-Leibler est\n",
    "alors donnée par:\n",
    "\n",
    "$$\n",
    "KL(q_\\phi(\\boldsymbol z | \\boldsymbol x) || p_\\theta(\\boldsymbol z)) = \\frac{1}{2} \\bigl ( \\text{tr}(\\boldsymbol \\sigma \\boldsymbol I) + \\boldsymbol \\mu^T \\boldsymbol \\mu - k - \\log \\text{det}(\\boldsymbol \\sigma \\boldsymbol I)  \\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27ddb7",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Modifier la fonction `vae_loss()` ci-dessous de sorte à calculer la fonction de coût du $ \\beta $-auto-encodeur variationnel (on souhaite pouvoir changer $ \\beta $ facilement plus tard si besoin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f405d",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9f613",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "\n",
    "def vae_loss(hat_x, x, mu, logvar):\n",
    "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + beta * kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8b6f2",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "\n",
    "Ce travail effectué, nous disposons du modèle, de la fonction objectif\n",
    "et des données. Il ne reste plus qu’à réaliser l’apprentissage du VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b161c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def train_vae(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    # Création du DataLoader pour charger les données\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    # Choix de la fonction de coût\n",
    "    criterion = vae_loss\n",
    "    # Passe le modèle en mode \"apprentissage\"\n",
    "    net = net.to(device)\n",
    "    net = net.train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        # Parcours du dataset pour une epoch\n",
    "        for images, _ in tqdm(train_dataloader):\n",
    "            # les labels sont ignorés pour l'apprentissage de l'auto-encodeur\n",
    "\n",
    "            images = images.to(device)\n",
    "            # Calcul de la reconstruction\n",
    "            reconstructions, latent_mu, latent_logvar = net(images)\n",
    "            # Calcul de l'erreur\n",
    "            loss = criterion(reconstructions, images, latent_mu, latent_logvar)\n",
    "\n",
    "            # Rétropropagation du gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Descente de gradient (une itération)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf9430a",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Créer un modèle de VAE et lancer son apprentissage à l’aide de la fonction `train_vae` ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f3306",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd593fc5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vae = VariationalAutoencoder(latent_dimension)\n",
    "train_vae(vae, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f052a8b",
   "metadata": {},
   "source": [
    "### Visualisation des reconstruction\n",
    "\n",
    "Comme dans le cas de l’auto-encodeur classique, nous pouvons visualiser\n",
    "quelques reconstructions pour juger qualitativement des performances du\n",
    "modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661785d6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vae = vae.to(\"cpu\")\n",
    "images, _ = iter(test_dataloader).next()\n",
    "\n",
    "# Images de test\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Images du jeu de test bruitées\")\n",
    "show_grid(make_grid(images[1:50],10,5))\n",
    "\n",
    "# Reconstruction et visualisation des images reconstruites\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Reconstruction par l'auto-encodeur\")\n",
    "show_grid(visualize_reconstructions(vae, images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb1d49",
   "metadata": {},
   "source": [
    "### Echantillonnage dans l’espace latent\n",
    "\n",
    "Un VAE peut générer de nouvelles données à partir de vecteur tirés dans\n",
    "l’espacee latent. Plus le poids donné à la KL-divergence est important\n",
    "($ \\beta $ élevé), plus cet espace est “rempli”, au sens où la\n",
    "distribution de l’espace latent se rapproche d’une loi normale centrée\n",
    "réduite. Cette capacité d’interpolation augmentée vient en contrepartie\n",
    "de reconstructions pouvant être de significativement moins bonne qualité\n",
    "que des auto-encodeurs classiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466dadb2",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Échantillonner quelques vecteurs dans l’espace latent selon une loi centrée réduite. Décoder ces vecteurs pour obtenir des nouvelles images synthétiques. Vous pouvez passer par l’attribut `.decoder` de l’objet VAE pour accéder directement au décodeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f966b",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ed9c2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Échantillonnage selon une loi normale\n",
    "    latent = torch.randn(100, latent_dimension, device=device)\n",
    "\n",
    "    # Reconstruction\n",
    "    fake_images = vae.decoder(latent).cpu()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "show_grid(make_grid(fake_images[1:50],10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d59580",
   "metadata": {},
   "source": [
    "### Interpolation dans l’espace latent\n",
    "\n",
    "Comme pour l’auto-encodeur classique, il est possible d’interpoler entre\n",
    "deux vecteurs arbitraires dans l’espace latent. Pour un auto-encodeur\n",
    "normal, cette interpolation produit rarement des images plausibles car\n",
    "l’espace latent est discontinu et contient de nombreux “trous”. En\n",
    "revanche, ce n’est pas le cas du VAE grâce au prior gaussien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975bb18",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Interpoler entre deux vecteurs aléatoires dans l’espace latent et visualiser le résultat (par exemple, 10 images sur le segment entre $ z_1 $ et $ z_2 $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0f834",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3276b2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "z1 = torch.randn(1, latent_dimension, device=device)\n",
    "z2 = torch.randn(1, latent_dimension, device=device)\n",
    "\n",
    "n_steps = 10\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "for idx, alpha in enumerate(np.linspace(0, 1, n_steps + 1)):\n",
    "    z = (1 - alpha) * z1 + alpha * z2\n",
    "    with torch.no_grad():\n",
    "        fake_image = vae.decoder(z)[0,0,:,:].cpu().numpy()\n",
    "\n",
    "    fig.add_subplot(1, n_steps + 1, idx + 1)\n",
    "    plt.imshow(fake_image)\n",
    "    plt.title(f\"$\\\\alpha = {alpha:0.1f}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e826ca",
   "metadata": {},
   "source": [
    "### Visualisation de l’espace latent\n",
    "\n",
    "Nous pouvons comparer l’espace latent de l’auto-encodeur classique et\n",
    "celui de l’auto-encodeur variationnel, par exemple à l’aide de t-SNE\n",
    "pour projeter les codes latents du jeu de test sur un plan en deux\n",
    "dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b153298",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "*(optionnel, pour l’approfondissement)* Pour toutes les images du jeu de test de Fashion-MNIST, calculer le code latent associé (on prendra la moyenne de la distribution dans le cas du VAE). Appliquer une réduction de dimension non-linéaire en utilisant la version de [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) disponible dans scikit-learn pour projeter les codes latents dans le plan. Coloriez les points en fonction de leur catégorie dans le jeu de données.\n",
    "\n",
    "Que constatez-vous ? Que se passe-t-il si l’on refait l’expérience avec une valeur plus élevée pour $ \\beta $ dans le VAE ? Avec une valeur plus faible ?"
   ]
  }
 ],
 "metadata": {
  "date": 1649691302.1922512,
  "filename": "TP7-AEetVAE.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python"
  },
  "title": "Travaux pratiques : auto-encodeurs variationnels"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}