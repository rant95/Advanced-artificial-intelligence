{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab093d2",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpcvae'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3d6b6",
   "metadata": {},
   "source": [
    "# Travaux pratiques : VAE conditionnel et PixelCNN\n",
    "\n",
    "L’objectif de cette séance de travaux pratiques est d’utiliser d’une\n",
    "part la génération conditionnée de chiffres à l’aide d’un autoencoder\n",
    "variationnel conditionnel et d’autre part d’illustrer la mise en œuvre\n",
    "d’un modèle autorégressif sur des images, en l’occurrence PixelCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70bf6b",
   "metadata": {},
   "source": [
    "## Préambule\n",
    "\n",
    "Comme les semaines passées, nous pouvons commencer par importer quelques\n",
    "bibliothèques utiles (Matplotlib, NumPy et PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5e0a7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Import des bibliothèques utiles\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7924615",
   "metadata": {},
   "source": [
    "Il est préférable d’utiliser une machine dotée d’une carte graphique\n",
    "(GPU) pour ce TP afin d’accélérer les calculs et ne de pas devoir\n",
    "patienter trop longtemps lors de l’entraînement des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62927a10",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Utilise la carte graphique (si disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68418d8",
   "metadata": {},
   "source": [
    "## VAE conditionnel\n",
    "\n",
    "Pour commencer, nous allons implémenter un autoencodeur variationnel\n",
    "conditionnel en nous inspirant du VAE que nous avons écrit lors du\n",
    "précédent TP. À titre d’illustration, nous allons expérimenter avec la\n",
    "base de données MNIST qui contient des chiffres manuscrits étiquetés de\n",
    "0 à 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583adc0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114c8b",
   "metadata": {},
   "source": [
    "On souhaite dans notre cas conditionner le modèle au chiffre représenté\n",
    "par l’image. Cette information correspond dans le jeu de données à une\n",
    "étiquette sous la forme d’un entier de 0 à 9, que l’on représentera plus\n",
    "tard sous la forme d’un vecteur en encodage *one-hot*, c’est-à-dire :\n",
    "`[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]` pour l’étiquette $ 6 $.\n",
    "\n",
    "La longueur du vecteur de conditionnement est donc 10 (le nombre de\n",
    "classes du jeu de données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c956690",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dee97c",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Reprendre le code du VAE du TP précédent et l’adapter de sorte à ce que\n",
    ":\n",
    "\n",
    "- l’encodeur accepte le vecteur de conditionnement comme deuxième\n",
    "  argument dans la méthode `.forward()`. Il faudra notamment décider\n",
    "  de l’endroit où injecter le vecteur de conditionnement (on pourra,\n",
    "  par exemple, le concaténer au vecteur aplati des *features* obtenu en\n",
    "  sortie des couches convolutives).  \n",
    "- le décodeur accepte le vecteur de conditionnement comme deuxième\n",
    "  argument dans la méthode `.forward()`. On se contentera de\n",
    "  concaténer le conditionnement au code latent `z`.  \n",
    "\n",
    "\n",
    "Attention, il ne faut pas oublier de modifier les dimensions des couches\n",
    "entièrement connectées lorsque cela est nécessaire. En effet, l’espace\n",
    "latent « change » de dimension lorsque que l’on concatène le vecteur de\n",
    "conditionnement au code latent…\n",
    "\n",
    "**Indice** : la méthode `torch.cat()` et sa\n",
    "[documentation](https://pytorch.org/docs/stable/generated/torch.cat.html)\n",
    "pourront vous être utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5becff",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a013cf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Flatten(),\n",
    "                                   )\n",
    "        self.linear1 = nn.Linear(in_features=64*7*7+num_classes, out_features=latent_dimension)\n",
    "        self.linear2 = nn.Linear(in_features=64*7*7+num_classes, out_features=latent_dimension)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        x_mu = self.linear1(x)\n",
    "        x_logvar = self.linear2(x)\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=latent_dimension+num_classes, out_features=64*7*7)\n",
    "        self.model = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        z = torch.cat((z, c), dim=1)\n",
    "        hat_x = F.relu(self.linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.model(hat_x)\n",
    "        return hat_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be2363",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "De la même façon, modifier l’implémentation de l’autoencodeur variationnel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a436de2",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d580bd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        latent_mu, latent_logvar = self.encoder(x, c)\n",
    "        z = self.latent_sample(latent_mu, latent_logvar)\n",
    "        hat_x = self.decoder(z, c)\n",
    "        return hat_x, latent_mu, latent_logvar\n",
    "\n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a64c3",
   "metadata": {},
   "source": [
    "La boucle d’apprentissage ne change presque pas par rapport au TP\n",
    "précédent : il suffit d’encoder les étiquettes dans le format *one-hot*\n",
    "et de les fournir au VAE au moment du calcul de la reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e3213",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def vae_loss(hat_x, x, mu, logvar):\n",
    "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "def train_vae(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    # Création du DataLoader pour charger les données\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    # Choix de la fonction de coût\n",
    "    criterion = vae_loss\n",
    "    # Passe le modèle en mode \"apprentissage\"\n",
    "    net = net.to(device)\n",
    "    net = net.train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        # Parcours du dataset pour une epoch\n",
    "        for images, labels in tqdm(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            # Encodage one-hot des labels\n",
    "            labels = F.one_hot(labels, num_classes=10).to(device)\n",
    "\n",
    "            # Calcul de la reconstruction\n",
    "            reconstructions, latent_mu, latent_logvar = net(images, labels)\n",
    "            # Calcul de l'erreur\n",
    "            loss = criterion(reconstructions, images, latent_mu, latent_logvar)\n",
    "\n",
    "            # Rétropropagation du gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Descente de gradient (une itération)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net.to(\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746ade6",
   "metadata": {},
   "source": [
    "Nous pouvons ainsi entraîner notre VAE conditionnel sur la base de\n",
    "données MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b191143",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vae = VariationalAutoencoder(10)\n",
    "vae = train_vae(vae, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a494a5",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Générer et visualiser des chiffres de différentes classes en\n",
    "échantillonnant dans la distribution conditionnelle. Le code latent est\n",
    "toujours échantillonné selon une loi normale\n",
    "$ \\mathcal{N}(0, \\mathbf{I}) $, seul le vecteur de conditionnement\n",
    "change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65976a2",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271b834",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "digit = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Échantillonnage selon une loi normale\n",
    "    latent = torch.randn(100, 10)\n",
    "    # Conditionnement\n",
    "    condition = torch.zeros(100, 10, dtype=int)\n",
    "    condition[:, digit] = 1\n",
    "\n",
    "    # Reconstruction\n",
    "    fake_images = vae.decoder(latent, condition).cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    show_image(torchvision.utils.make_grid(fake_images.data[:100],10,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c980815",
   "metadata": {},
   "source": [
    "## PixelCNN\n",
    "\n",
    "[PixelCNN](https://arxiv.org/abs/1601.06759) est un modèle\n",
    "autorégressif permettant d’estimer la densité d’un jeu de données\n",
    "d’images. La différence essentielle entre PixelCNN et un CNN\n",
    "traditionnel est l’application de masques sur les noyaux de convolution.\n",
    "Ceux-ci permettent de cacher l’information des pixels non encore\n",
    "rencontrés par le modèle afin de ne pas briser le processus\n",
    "autorégressif :\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\prod_{i=1}^{n^2} p(x_i|x_1, \\dots, x_{i-1})\n",
    "$$\n",
    "\n",
    "Dans l’article de PixelCNN, deux types de masques sont introduits :\n",
    "\n",
    "- les masques de type `'A'` ne perçoivent que le quart nord-ouest du\n",
    "  noyau de convolution sans le pixel central,  \n",
    "- les masques de type `'B'` perçoivent égalemment le pixel central.  \n",
    "\n",
    "\n",
    "En pratique, la première couche de convolution masquée est de type A\n",
    "tandis que les couches suivantes sont de type B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58a10b",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Expliquer brièvement l’intérêt du masque de type A et pourquoi il est nécessaire que la première couche du modèle soit de ce type.\n",
    "\n",
    "Pour cet exemple, nous allons utiliser le jeu de données FashionMNIST\n",
    "(mais vous pouvez conserver le jeu de données MNIST si vous préférez, ce\n",
    "dernier étant d’ailleurs plus facile à modéliser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba80ca",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=ToTensor())\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3827e9",
   "metadata": {},
   "source": [
    "Les convolutions masquées de type A et B n’existent pas nativement dans\n",
    "PyTorch, il est donc nécessaire que nous les implémentions par\n",
    "nous-mêmes. Nous allons définir un nouveau module `MaskedConvolution`\n",
    "qui hérite de la convolution 2D standard de PyTorch.\n",
    "\n",
    "Dans une convolution 2D `nn.Conv2d`, les poids des filtres convolutifs\n",
    "sont stockés dans l’attribut `.weight.data`. Nous allons donc\n",
    "introduire un nouvel attribut `.weight.mask` de même dimensions que\n",
    "les filtres de convolution mais qui contiendra des entrées binaires : 1\n",
    "pour les pixels que l’on doit conserver, 0 pour les pixels que l’on doit\n",
    "masquer.\n",
    "\n",
    "<img src=\"images/pixelcnn_masque_a.png\" alt=\"Masque de convolution de type A\" style=\"\">\n",
    "\n",
    "Masque de convolution de type A  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072ac64",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Implémenter les nouveaux noyaux de convolution avec les masques correspondants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab85ae",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8c402",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConvolution, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Vérifier que le type de masque est autorisé\n",
    "        assert mask_type in ['A', 'B'], \"Type invalide\"\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "\n",
    "        _, depth, height, width = self.weight.size()\n",
    "\n",
    "        # Poids de la convolution :\n",
    "        # on débute avec des 1 partout (tous les pixels sont considérés)\n",
    "        self.mask.fill_(1)\n",
    "\n",
    "        if mask_type == 'A':\n",
    "            self.mask[:,:, height//2 ,width//2:] = 0\n",
    "            self.mask[:,:, height//2 + 1:,:] = 0\n",
    "        elif mask_type == 'B':\n",
    "            self.mask[:,:, height//2, width//2 + 1:] = 0\n",
    "            self.mask[:,:, height//2 + 1:,:] = 0\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Le filtre convolutif est \"masqué\" par multiplication avec le masque binaire\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConvolution, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72b97e",
   "metadata": {},
   "source": [
    "Le modèle PixelCNN que nous allons implémenter s’inspire fortement de\n",
    "celui qui est décrit dans l’article original. Il s’agit d’un simple\n",
    "enchaînement de convolutions (masquées), de *BatchNorm* et d’activations\n",
    "non-linéaires (*ReLU*). En réalité, la seule différence avec un CNN\n",
    "habituel est l’utilisation du module `MaskedConvolution` à la place\n",
    "des couches `nn.Conv2d` classiques.\n",
    "\n",
    "On considèrera que les images en niveaux de gris (valeurs entre 0 et\n",
    "255) correspondent à des images de 256 classes. En sortie, nous aurons\n",
    "donc une prédiction par pixel, chaque prédiction correspondant à un\n",
    "vecteur de logits (avant *softmax*) de 256 valeurs. La sortie est donc\n",
    "une carte d’activations $ (28\\times 28 \\times 256) $ (avec les mêmes\n",
    "dimensions spatiales que l’image d’entrée, puisque l’on prédit la valeur\n",
    "de chaque pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a3239",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Implémenter le modèle suivant:\n",
    "\n",
    "- Convolution masquée de type A (1 canal d’entrée, `channels` canaux\n",
    "  de sortie, noyau de dimension 7, `stride=1`, `padding=3`) +\n",
    "  *BatchNorm* + *ReLu*  \n",
    "- Convolution masquée de type B (`channels` canaux d’entrée,\n",
    "  `channels` canaux de sortie, noyau de dimension 3, `stride=1`,\n",
    "  `padding=1`) + *BatchNorm* + *ReLU*  \n",
    "  - à répéter **7 fois**  \n",
    "- Convolution classique (`channels` canaux d’entrée, `classes`\n",
    "  canaux de sortie, `kernel_size=1` (pour mettre la sortie dans la\n",
    "  bonne dimension)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2722683",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfac9d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    PixelCNN implémenté dans l'esprit de van den Oord et al., 2016 (https://arxiv.org/abs/1601.06759)\n",
    "    \"\"\"\n",
    "    def __init__(self, classes=256, channels=64):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        def conv_block(mask_type, in_channels, out_channels, kernel):\n",
    "            return nn.Sequential(\n",
    "                MaskedConvolution(mask_type, in_channels, out_channels, kernel_size=kernel, stride=1, padding=kernel//2, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv_block1 = conv_block('A', 1, channels, 7)\n",
    "\n",
    "        self.conv_block2 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block3 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block4 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block5 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block6 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block7 = conv_block('B', channels, channels, 3)\n",
    "        self.conv_block8 = conv_block('B', channels, channels, 3)\n",
    "\n",
    "        self.out = nn.Conv2d(channels, classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        x = self.conv_block6(x)\n",
    "        x = self.conv_block7(x)\n",
    "        x = self.conv_block8(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3bbbf",
   "metadata": {},
   "source": [
    "A présent nous allons entraîner ce modèle sur FashionMNIST. Comme nous\n",
    "considérons que la distribution est pixels est discrète, la fonction de\n",
    "coût est l’entropie croisée, comme d’habitude lorsque nous avons affaire\n",
    "à une tâche de classification. À noter que dans notre cas, l’entropie\n",
    "croisée est moyennée sur les prédictions pour tous les pixels de l’image\n",
    "autorégressée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f0a8e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def train(net, train_dataset, epochs=5, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    # Création du DataLoader pour charger les données\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "    # Choix de la fonction de coût (entropie croisée)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Passe le modèle en mode \"apprentissage\"\n",
    "    net = net.to(device)\n",
    "    net = net.train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        # Parcours du dataset pour une epoch\n",
    "        for images, _ in tqdm(train_dataloader):\n",
    "            # les labels sont ignorés pour l'apprentissage de l'auto-encodeur\n",
    "\n",
    "            images = images.to(device)\n",
    "            # Conversion en 256 classes\n",
    "            target = (images[:,0]*255).long().to(device)\n",
    "\n",
    "            # Calcul de la reconstruction\n",
    "            reconstructions = net(images)\n",
    "            # Calcul de l'erreur\n",
    "            loss = F.cross_entropy(reconstructions, target)\n",
    "\n",
    "            # Rétropropagation du gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Descente de gradient (une itération)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            t.set_description(f\"Epoch {epoch}: loss = {loss.item():.3f}\")\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f7e12",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "net = PixelCNN()\n",
    "net = train(net, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e34565",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Calculer et visualiser les reconstructions de quelques exemples du jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd3129",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cd51d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "images, labels = iter(test_dataloader).next()\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = model(images)\n",
    "        images = images.cpu()\n",
    "        images = torch.argmax(images,dim=1).unsqueeze(dim=1)/255\n",
    "        images = to_img(images)\n",
    "        print(images.shape)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()\n",
    "# First visualise the original images\n",
    "print('Original images')\n",
    "plt.figure(figsize=(12, 6))\n",
    "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
    "plt.show()\n",
    "\n",
    "# Reconstruct and visualise the images using the vae\n",
    "print('VAE reconstruction:')\n",
    "plt.figure(figsize=(12, 6))\n",
    "visualise_output(images, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2218c",
   "metadata": {},
   "source": [
    "Ce modèle PixelCNN peut être utilisé pour générer séquentiellement des\n",
    "images synthétiques respectant les probabilités conditionnelles\n",
    "apprises. Pour ce faire, il est nécessaire de partir d’une image vide\n",
    "(contenant uniquement des zéros) puis d’inférer le premier pixel, et\n",
    "ainsi de suite.\n",
    "\n",
    "On peut compléter de cette façon l’image colonne par colonne puis ligne\n",
    "par ligne en utilisant le modèle afin de prédire la valeur du pixel\n",
    "suivant (c’est-à-dire la probabilité suivante du processus\n",
    "autorégressif)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac48b9",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Parcourir l’image synthétique pour la remplir pixel par pixel. Il sera\n",
    "nécessaire d’effectuer une inférence à chaque ajout d’un nouveau pixel.\n",
    "Les probabilités des classes (c’est-à-dire la valeur du pixel de 0 à\n",
    "255) sera obtenue en appliquant une activation *softmax* sur les scores\n",
    "bruts en sortie du modèle. Cela permettra d’obtenir la distribution\n",
    "multinomiale dans laquelle échantillonner la valeur du pixel suivant.\n",
    "\n",
    "Il est également possible de choisir systématiquement la classe la plus\n",
    "probable mais ce procédé déterministe ne permettra de générer qu’une\n",
    "seule image (toujours la même)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b2b49",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787e227",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sample = torch.zeros((128, 1, 28, 28)).to(device)\n",
    "\n",
    "net.eval()\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        out = net(sample)\n",
    "        probs = F.softmax(out[:, :, i, j],dim=1).data\n",
    "        sample[:, :, i, j] = torch.multinomial(probs, 1).float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83cc05",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "show_image(torchvision.utils.make_grid(sample.cpu().data[:100],10,5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "date": 1651667240.968303,
  "filename": "TP8-CVAE_PixelCNN.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python"
  },
  "title": "Travaux pratiques : VAE conditionnel et PixelCNN"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}