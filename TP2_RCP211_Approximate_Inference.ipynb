{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TP2_RCP211_Approximate_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfDd0DKVtknP"
      },
      "source": [
        "# TP 2: Approximate Inference in Classification\n",
        "\n",
        "In classification taks, even for a mere Logistic Regression, we don't have access to a closed form of the posterior $p(\\pmb{w} \\vert \\mathcal{D})$. Unlike in Linear regression, the likelihood isn't conjugated to the Gaussian prior anymore. We ill need to approximate this posterior.\n",
        "\n",
        "During this session, we will explore and compare approximate inference approaches on 2D binary classification datasets. Studied approaches include Laplacian approximation, variational inference with mean-field approximation and Monte Carlo dropout.\n",
        "\n",
        "**Goal**: Take hand on approximate inference methods and understand how they works on linear and non-linear 2D datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clhZGFZ2wOxg"
      },
      "source": [
        "### All Imports and Useful Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Cv7C3O7qaw"
      },
      "source": [
        "Here we are going to install and import everything we are going to need for this tutorial. \n",
        "\n",
        "**Note**: *You can double-click the title of the collapsed cells (as the ones below) to expand them and read their content.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-kN2Wxbv2Ui"
      },
      "source": [
        "#@title Import libs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "import torch.distributions as dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMDnycZQPMpL"
      },
      "source": [
        "#@title Useful plot function \n",
        "def plot_decision_boundary(model, X, Y, epoch, accuracy, model_type='classic', \n",
        "                           nsamples=100, posterior=None, tloc=(-4,-7), \n",
        "                           nbh=2, cmap='RdBu'):    \n",
        "    \"\"\" Plot and show learning process in classification \"\"\"\n",
        "    h = 0.02*nbh\n",
        "    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
        "    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min*2, x_max*2, h),\n",
        "                         np.arange(y_min*2, y_max*2, h))\n",
        "    \n",
        "    test_tensor = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).type(torch.FloatTensor)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      if model_type=='classic':\n",
        "          pred = model(test_tensor)\n",
        "      elif model_type=='laplace':\n",
        "          #Save original mean weight\n",
        "          original_weight = model.state_dict()['fc.weight'].detach().clone()\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              state_dict = model.state_dict()\n",
        "              state_dict['fc.weight'] = torch.from_numpy(posterior[i].reshape(1,2))\n",
        "              model.load_state_dict(state_dict)\n",
        "              outputs[i] = net(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "          state_dict['fc.weight'] = original_weight\n",
        "          model.load_state_dict(state_dict)\n",
        "      elif model_type=='vi':\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              outputs[i] = model(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "      elif model_type=='mcdropout':\n",
        "          model.eval()\n",
        "          model.training = True\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              outputs[i] = model(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "    Z = pred.reshape(xx.shape).detach().numpy()\n",
        "\n",
        "    plt.cla()\n",
        "    ax.set_title('Classification Analysis')\n",
        "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n",
        "    ax.contour(xx, yy, Z, colors='k', linestyles=':', linewidths=0.7)\n",
        "    ax.scatter(X[:,0], X[:,1], c=Y, cmap='Paired_r', edgecolors='k');\n",
        "    ax.text(tloc[0], tloc[1], f'Epoch = {epoch+1}, Accuracy = {accuracy:.2%}', fontdict={'size': 12, 'fontweight': 'bold'})\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAk_gzoRtknr"
      },
      "source": [
        "## Part I: Bayesian Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4aNuqE77jsS"
      },
      "source": [
        "In linear regression, model prediction is of the continuous form $f(\\pmb{x})=\\pmb{w}^T\\pmb{x}+b$.\n",
        "\n",
        "For classification, we wish to predict discrete class labels $\\mathcal{C}_k$ to a sample $\\pmb{x}$. \n",
        "For simplicity, let's consider here binary classification:\n",
        "$$f(\\pmb{x}) = \\sigma(\\pmb{w}^T\\pmb{x} + b)$$\n",
        "where $\\sigma(t)= \\frac{1}{1+e^t}$ is the sigmoid function.\n",
        "\n",
        "As in linear regression, we define a Gaussian prior: \n",
        "$$ p(\\pmb{w}) = \\mathcal{N}(\\pmb{w}; \\pmb{\\mu}_0, \\pmb{\\Sigma}_0^2) $$\n",
        "Unfortunately, the posterior distribution isn't tractable as the likelihood isn't conjugate to the prior anymore.\n",
        "\n",
        "We will explore in the following different methods to obtain an estimate of the posterior distribution and hence the predictive distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCp4ytXhV6IU"
      },
      "source": [
        "### I.0 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-QrdljAUG5E"
      },
      "source": [
        "#@title Hyperparameters for model and approximate inference { form-width: \"30%\" }\n",
        "\n",
        "WEIGHT_DECAY = 5e-2 #@param\n",
        "NB_SAMPLES = 400 #@param\n",
        "TEXT_LOCATION = (-5,-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPI60gDOtkns"
      },
      "source": [
        "# Load linear dataset\n",
        "X, y = make_blobs(n_samples=NB_SAMPLES, centers=[(-2,-2),(2,2)], cluster_std=0.80, n_features=2)\n",
        "X, y = torch.from_numpy(X), torch.from_numpy(y)\n",
        "X, y = X.type(torch.float), y.type(torch.float)\n",
        "torch_train_dataset = data.TensorDataset(X,y) # create your datset\n",
        "train_dataloader = data.DataLoader(torch_train_dataset, batch_size=len(torch_train_dataset))\n",
        "\n",
        "# Visualize dataset\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired_r', edgecolors='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNGfQhFqtknu"
      },
      "source": [
        "### I.1 Maximum-A-Posteriori Estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CE5ewMVdt9"
      },
      "source": [
        "\n",
        "In this \"baseline\", we reduce our posterior distribution $p(\\pmb{w} | \\mathcal{D})$ to a point estimate $\\pmb{w}_{MAP}$. For a new sample $\\pmb{x^*}$, the predictive distribution can then be approximated by\n",
        "$$ p(\\mathbf{y} = 1|\\pmb{x^*},\\mathcal{D}) = \\int p(\\mathbf{y} =1 |\\pmb{x},\\pmb{w})p(\\pmb{w} | \\mathcal{D})d\\pmb{w} \\approx p(y =1 |\\pmb{x},\\pmb{w}_{\\textrm{MAP}}).$$\n",
        "This approximation is called the **plug-in approximation**.\n",
        "\n",
        "The point estimate corresponds to the Maximum-A-Posteriori minimum given by:\n",
        "$$ \\pmb{w}_{\\textrm{MAP}} = arg \\max_{\\pmb{w}} p(\\pmb{w} \\vert \\mathcal{D}) = arg \\max_{\\pmb{w}} p(\\mathcal{D} \\vert \\pmb{w})p(\\pmb{w}) = arg \\max_{\\pmb{w}} \\prod_{n=1}^N p(y_n \\vert \\pmb{x}_n, \\pmb{w})p(\\pmb{w}) $$\n",
        "Looking for the maximum solution of previous equation is equivalent to the minimum solution of $- \\log p(\\pmb{w} \\vert \\mathcal{D})$. In case of a Gaussian prior, it can further be derived as:\n",
        "$$ \\pmb{w}_{\\textrm{MAP}} = arg \\min_{\\pmb{w}} \\sum_{n=1}^N \\big ( -y_n \\log \\sigma(\\pmb{w}^T \\pmb{x}_n + b) - (1-y_n) \\log (1 - \\sigma(\\pmb{w}^T \\pmb{x}_n + b)) + \\frac{1}{2 \\Sigma_0^2} \\vert \\vert \\pmb{w} \\vert \\vert_2^2 \\big ) $$\n",
        "\n",
        "Note that:\n",
        "- This actually correspond to the minimum given by the standard **cross-entropy** loss in classification with a weight decay regularization\n",
        "- Unlike in linear regression, $\\pmb{w}_{MAP}$ **cannot be computed analytically**\n",
        "- But we can use optimization methods to compute it, e.g. **stochastic gradient descent**\n",
        "- Nevertheless, we only obtain a **point-wise estimate**, and not a full distribution over parameters $\\pmb{w}$\n",
        "\n",
        "\n",
        "Consequently, **the objective is simply to implement and train a Logistic Regression model** with Pytorch and then compute $p(\\mathbf{y} = 1|\\pmb{x}^*,\\mathcal{D})$ on a new sample $\\pmb{x}^*$ as in a deterministic model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[CODING TASK]** Implement Logistic Regression"
      ],
      "metadata": {
        "id": "ChS5Md3se3xT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_I-DiH3tknv"
      },
      "source": [
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  \"\"\" A Logistic Regression Model with sigmoid output in Pytorch\"\"\"\n",
        "  def __init__(self, input_size):\n",
        "    super().__init__()\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    # cf. nn.Linear()\n",
        "    # self.fc = n.Linear(...)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    # Don't forget to apply the sigmoid function when returning the output\n",
        "    # cf. torch.sigmoid()\n",
        "\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[CODING TASK]** Train a Logistic Regression model with stochastic gradient descent for 20 epochs."
      ],
      "metadata": {
        "id": "5RlsSHkPe6YM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx-vyfGdtknw"
      },
      "source": [
        "net = LogisticRegression(input_size=X.shape[1])\n",
        "net.train()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# L2 regularization is included in Pytorch's optimizer implementation\n",
        "# as \"weigth_decay\" option\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# Train previously defined network for 20 epochs with SGD \n",
        "# and plot result for each epoch by uncommenting function below\n",
        "\n",
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    # output = net(X).squeeze() # rwhere W is the input\n",
        "    # loss = criterion(output,y) # where y is the label\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # For plotting and showing learning process at each epoch\n",
        "    # plot_decision_boundary(net, X, y, epoch, ((output>=0.5) == y).float().mean(), \n",
        "    #                       model_type='classic', tloc=TEXT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkFuifc5tknx"
      },
      "source": [
        "**[Question 1.1]: Analyze the results provided by previous plot. Looking at $p(\\mathbf{y}=1 | \\pmb{x}, \\pmb{w}_{\\textrm{MAP}})$, what can you say about points far from train distribution?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTozn8kQccU"
      },
      "source": [
        "### I.2 Laplace Approximation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BadWX2XfWEZe"
      },
      "source": [
        "We will use Laplace approximation to estimate the intractable posterior $p(\\pmb{w} | \\mathcal{D})$.\n",
        "\n",
        "Here, $p(\\pmb{w} | \\mathcal{D})$ is approximated with a normal distribution $\\mathcal{N}(\\pmb{w} ; \\pmb{\\mu}_{lap}, \\pmb{\\Sigma}_{lap}^2)$ where: \n",
        "\n",
        "- the mean of the normal distribution $\\pmb{\\mu}_{lap}$ corresponds to the mode of $p(\\pmb{w} | \\mathcal{D})$. In other words, it simply consists in taking the optimum weights of Maximum-A-Posteriori estimation : \n",
        "$$\\pmb{\\mu}_{lap} = \\pmb{w}_{\\textrm{MAP}} = \\arg \\min_{\\pmb{w}} -\\log p(\\pmb{w} | \\mathcal{D})$$. \n",
        "- the covariance matrix is obtained by computing the Hessian of the loss function $-\\log p(\\pmb{w} \\vert \\mathcal{D})$ at $\\pmb{w}=\\pmb{w}_{\\textrm{MAP}}$: \n",
        "$$(\\pmb{\\Sigma}^2_{lap})^{-1} = \\nabla\\nabla_{\\pmb{w}} [p(\\pmb{w} \\vert \\mathcal{D}) ]_{\\pmb{w}=\\pmb{w}_{\\textrm{MAP}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[CODING TASK]** Extract μ_lap from previously trained model. *NB: Select only weights parameters. The weights of a given layer `l_id` from a model `net` is accessible as `net.l_id.weight`*"
      ],
      "metadata": {
        "id": "NSIZiAYueq5U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LrZKf_dUs5q"
      },
      "source": [
        "# ============ YOUR CODE HERE ============\n",
        "w_map = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brSZS4nZx4_U"
      },
      "source": [
        "To compute the Hessian, we first compute the gradient at $\\pmb{w}_{\\textrm{MAP}}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEOmQfiHS6L-"
      },
      "source": [
        "# Computing first derivative w.r.t to model's weights\n",
        "optimizer.zero_grad()\n",
        "output = net(X).squeeze()\n",
        "loss = criterion(output, y) + net.fc.weight.norm()**2\n",
        "gradf_weight = grad(loss, net.fc.weight, create_graph=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADH0i667exQY"
      },
      "source": [
        "# Apply the same grad function on each scalar element of the gradient to get \n",
        "# each raw of the Hessian. Concatenate both and compute the covariance \n",
        "# by inverting the Hessian\n",
        "# NB: to avoid accumulated gradient when debugging and running the cell \n",
        "# multiple times, you should convert your grad results to numpy straight away\n",
        "hess_weights = []\n",
        "for i in range(gradf_weight.shape[1]):\n",
        "    grad2 = grad(gradf_weight[0][i], net.fc.weight, retain_graph=True)[0].squeeze()\n",
        "    hess_weights.append(grad2)\n",
        "hess_weights = torch.stack(hess_weights)\n",
        "Sigma_laplace = torch.inverse(hess_weights)\n",
        "print(hess_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g7CKY0Myq1o"
      },
      "source": [
        "We now compute the posterior approximate $\\mathcal{N}(\\pmb{w} ; \\pmb{\\mu}_{lap}, \\pmb{\\Sigma}_{lap}^2)$ with the parameters found. \n",
        "\n",
        "Given this distribution, we can compute the posterior thanks to Monte-Carlo sampling and plot results for the last epoch corresponding to $\\pmb{w}_{\\textrm{MAP}}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xf9fF5OuB_K"
      },
      "source": [
        "# Defining posterior distribution\n",
        "laplace_posterior =  np.random.multivariate_normal(w_map.detach().numpy().reshape(2,), Sigma_laplace.detach().numpy(), NB_SAMPLES)\n",
        "\n",
        "# Plotting results\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), model_type='laplace', \n",
        "                       tloc=TEXT_LOCATION, nsamples=NB_SAMPLES, posterior=laplace_posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUPjVjot4IKE"
      },
      "source": [
        "**[Question 1.2]: Analyze the results provided by previous plot. Compared to previous MAP estimate, how does the predictive distribution behave?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_scDA7bC1c"
      },
      "source": [
        "### II Monte Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8J9UNT41TJ2"
      },
      "source": [
        "Training a neural network with randomly dropping some activations, such as with dropout layers, can actually be seen as an **approximate variational inference method**!\n",
        "\n",
        "[Gal and Ghahramani, 2016] showed this can be fullfilled for:\n",
        "- $p(\\pmb{w}) = \\prod_l p(\\pmb{W}_l) = \\prod_l \\mathcal{N}(0, I/ l_i^2)$ $\\Rightarrow$ Multivariate Gaussian distribution factorized over layers\n",
        "- $q(\\pmb{w}) = \\prod_l q(\\pmb{W}_l) = \\prod_l \\textrm{diag}(\\varepsilon_l)\\odot\\pmb{M}_l $ with $\\varepsilon_l \\sim \\textrm{Ber}(1-p_l)$.\n",
        "\n",
        "We will now implement a MLP with dropout layers and perform Monte-Carlo sampling to obtain the predictive distribution $p(\\mathbf{y} \\vert \\pmb{x}^*, \\pmb{w})$ for a new sample $\\pmb{x}^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxGmkjd0kVgZ"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a MLP with dropout (p=0.2)\n",
        "# Code MLP with 1 hidden layer and a dropout layer. Be careful, the dropout \n",
        "# layer should be also activated during test time.\n",
        "# (Hint: we may want to look out at F.dropout())\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Pytorch MLP for binary classification model with an added dropout layer\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "      # ============ YOUR CODE HERE ============\n",
        "\n",
        "    def forward(self, x):\n",
        "      # ============ YOUR CODE HERE ============\n",
        "      # Don't forget to apply the sigmoid function when returning the output\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMlasuRN08Yk"
      },
      "source": [
        "We train our model as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqSZJDWIlf5e"
      },
      "source": [
        "net = MLP(input_size=X.shape[1], hidden_size=50)\n",
        "net.train()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "\n",
        "for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    output = net(X).squeeze()\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # For plotting and showing learning process at each epoch, uncomment and indent line below\n",
        "    if (epoch+1)%50==0:\n",
        "      plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='classic')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RyglMRa4d5"
      },
      "source": [
        "Now let's look at the results given by MC Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRh6BQDcazFx"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='mcdropout')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05OBuwko5EvS"
      },
      "source": [
        "**[Question 2.1]: Again, analyze the results showed on plot. What is the benefit of MC Dropout variational inference over Bayesian Logistic Regression with variational inference?**"
      ]
    }
  ]
}