{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCP211 : Deep Q-Networks_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkhbiyLrSUf6"
      },
      "source": [
        "# TP RL 3 : DQN\n",
        "\n",
        "Dans ce TP nous allons nous interesser au Deep Q Networks et leur implémentation pour une tâche classique de RL : le cartpole. Il s'agit d'apprendre à un agent à faire tenir un mât en équilibre sur un véhicule dans une scène 2D. L'agent à deux action possibles : gauche ou droite et reçoit une reward négative lorque le mat tombre de la voiture. Pour cela, nous allons utiliser la bibliothèque gym proposant toute sortes d'environnements et de benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pSuWToeSxf5"
      },
      "source": [
        "Commençons par charger quelques utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YclEG_TGRSl9"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdUCV_tCQ73K"
      },
      "source": [
        "import os\n",
        "\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from gym.wrappers import Monitor\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import pylab as pl\n",
        "from IPython import display as ipdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4IDCh6-dc7F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4MlrYuS18V"
      },
      "source": [
        "## PArtie 1 : DQN\n",
        "\n",
        "La classe DQN Agent gère l'essentiel des intéraction avec Gym et nous permettra d'avoir une vision abstraite de ce problème et de nous concentrer sur les modèles ainsi que sur leur utilisation.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btP3-9nfSJm4",
        "cellView": "form"
      },
      "source": [
        "#@title Class DQNAgent \n",
        "\n",
        "class DQNAgent(object):\n",
        "    def __init__(self, env_name, model, replay1d, update_target_model ,ddqn=False,Soft_Update=False, dueling=False,batch_size=128,gamma=0.95,memory_size=1e4):\n",
        "        \n",
        "        self.env_name = env_name       \n",
        "        self.env = self.wrap_env(gym.make(env_name))\n",
        "        self.env.seed(0)  \n",
        "        self.replay1d = replay1d\n",
        "        self.update_target_model = update_target_model\n",
        "\n",
        "        # by default, CartPole-v1 has max episode steps = 500\n",
        "        # we can use this to experiment beyond 500\n",
        "        self.env._max_episode_steps = 4000\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = 2000\n",
        "        \n",
        "        # Instantiate memory\n",
        "        memory_size = 10000\n",
        "        #self.MEMORY = Memory(memory_size)\n",
        "        self.memory = deque(maxlen=65536)\n",
        "\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        \n",
        "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
        "        self.epsilon = 1.0  # exploration probability at start\n",
        "        self.epsilon_min = 0.01  # minimum exploration probability \n",
        "        self.epsilon_decay = 0.0005  # exponential decay rate for exploration prob\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # defining model parameters\n",
        "        self.ddqn = ddqn # use doudle deep q network\n",
        "        self.Soft_Update = Soft_Update # use soft parameter update\n",
        "        self.dueling = dueling\n",
        "        self.epsilon_greedy = False # use epsilon greedy improved strategy\n",
        "        \n",
        "        self.TAU = .5 # target network soft update hyperparameter\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_PER_D3QN_CNN.h5\")\n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = model(in_channels=4,action_space = self.action_size, dueling = False, name = 'online model').to(device)\n",
        "        self.target_model = model(in_channels=4,action_space = self.action_size, dueling = False, name = 'target model').to(device) \n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(),lr=0.00025, eps=0.01, alpha=0.95)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.loss = 0\n",
        "        self.dd = torch.zeros(64, 4, 5, 5).to(device)\n",
        "        \n",
        "    def wrap_env(self,env):\n",
        "        env = Monitor(env, './video', force=True)\n",
        "        return env\n",
        "\n",
        "    # after some time interval update the target model to be the same as the online model\n",
        "    \n",
        "                    \n",
        "                \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        experience = state, action, reward, next_state, done\n",
        "        #print(f' state = {state}, action = {action}, reward = {reward}, next_state = {next_state}, done = {done}  ')\n",
        "        self.memory.append((experience))\n",
        "    \n",
        "    def act1d(self, state, decay_step):\n",
        "        # EPSILON GREEDY STRATEGY\n",
        "        if self.epsilon_greedy:\n",
        "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
        "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
        "        # OLD EPSILON STRATEGY\n",
        "        else:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= (1-self.epsilon_decay)\n",
        "            explore_probability = self.epsilon\n",
        "    \n",
        "        if explore_probability > np.random.rand():\n",
        "            # Make a random action (exploration)\n",
        "            q = random.randrange(self.action_size)\n",
        "            return int(np.argmax(q)), explore_probability\n",
        "\n",
        "        else:\n",
        "            # Get action from Q-network (exploitation)\n",
        "            # Estimate the Qs values state\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            q = self.model(torch.from_numpy(state).float().unsqueeze(0).to(device))\n",
        "            return int(torch.argmax(q)), explore_probability\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "    pylab.figure(figsize=(18, 9))\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "        dqn = 'DQN_'\n",
        "        softupdate = ''\n",
        "        dueling = ''\n",
        "        greedy = ''\n",
        "        PER = ''\n",
        "        if self.ddqn: dqn = 'DDQN_'\n",
        "        if self.Soft_Update: softupdate = '_soft'\n",
        "        if self.dueling: dueling = '_Dueling'\n",
        "        if self.epsilon_greedy: greedy = '_Greedy'\n",
        "        try:\n",
        "            pylab.savefig(dqn+self.env_name+softupdate+dueling+greedy+PER+\"_CNN.png\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        return str(self.average[-1])[:5]\n",
        "\n",
        "    \n",
        "    def reset1d(self):\n",
        "        self.env.reset()\n",
        "        return np.array([0,0,0,0])\n",
        "\n",
        "    \n",
        "    def step1d(self,action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    \n",
        "    def run1d(self):\n",
        "        decay_step = 0\n",
        "        loss_tab = []\n",
        "        dd = 0\n",
        "        dd_old = 0\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.reset1d()\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                decay_step += 1\n",
        "                action, explore_probability = self.act1d(state, decay_step)\n",
        "                next_state, reward, done, _ = self.step1d(action)\n",
        "\n",
        "                if not done or i == self.env._max_episode_steps-1:\n",
        "                    reward = reward\n",
        "                else:\n",
        "                    reward = -1e2\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                if done:\n",
        "                    if e % 10==0:\n",
        "                        update_target_model(self.Soft_Update,self.ddqn,self.model,self.target_model,self.TAU)\n",
        "                    average = self.PlotModel(i, e)\n",
        "                    loss_tab.append(self.loss)\n",
        "                    print(\"episode: {}/{}, score: {}, e: {:.2}, average: {}\\n\".format(e, self.EPISODES, i, explore_probability, average))\n",
        "                    if i == self.env._max_episode_steps:\n",
        "                        print(\"Saving trained model to\", self.Model_name)\n",
        "                        break\n",
        "                self.replay1d(self.memory,self.batch_size,self.model,self.target_model,self.ddqn,self.gamma,self.criterion,self.loss,self.optimizer)\n",
        "        plt.plot(loss_tab)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13SHck5usuF3"
      },
      "source": [
        "Pour commencer nous allons construire notre réseaux chargé d'approcher la fonction de valeur état-action $Q(s,a)$.\n",
        "\n",
        "### Coding task\n",
        "\n",
        "Compléter le code et implémenter le DQN suivant:\n",
        "\n",
        "* deux couches linéaires de dimensions (in_channels x 512), (512 x 256) et(256 x 64)\n",
        "* Activation ReLU après chaque couche "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMUV--GlR_OJ"
      },
      "source": [
        "class DQN1d(nn.Module):\n",
        "\n",
        "    def __init__(self, action_space, in_channels, dueling=false, name='NN'):\n",
        "        super(DQN1d, self).__init__()\n",
        "        self.name = name\n",
        "        self.dueling = dueling\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.head1 = nn.Linear(in_channels, 512)\n",
        "        self.head2 = nn.Linear(512, 256)\n",
        "        self.head3 = nn.Linear(256, 64)\n",
        "        \n",
        "        if dueling:\n",
        "            self.state_value = nn.Linear(64,1)\n",
        "            self.action_advantage = nn.Linear(64,action_space)\n",
        "        else:\n",
        "            self.Q = nn.Linear(64,action_space)\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        #print(f'{self.name} input = {x.shape}')\n",
        "        x = F.relu(self.head1(x))\n",
        "        x = F.relu(self.head2(x))\n",
        "        x = F.relu(self.head3(x))\n",
        "        \n",
        "        if self.dueling:\n",
        "            s = self.state_value(x).repeat((1,self.action_space))\n",
        "            a = self.action_advantage(x)\n",
        "            a -= torch.mean(a,dim=1, keepdim=True).repeat(1,2)\n",
        "            x = s+a\n",
        "        else:\n",
        "            x = self.Q(x)       \n",
        "        \n",
        "        #print(f'{self.name} output = {x.size()}')\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGMzd6tltaM5"
      },
      "source": [
        "Définissons ensuite notre fonction d'apprentissage, c'est à dire la fonction permettant d'extraires des minibatch du buffer d'expériences pour permettre l'apprentissage du réseau de nerones\n",
        "\n",
        "### Coding Task\n",
        "\n",
        "Compléter le code suivant pour implémenter le replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qD6vubfOPDx"
      },
      "source": [
        "def replay1d(memory,batch_size,model,target_model,ddqn,gamma,criterion,loss,optimizer):\n",
        "        # Randomly sample minibatch from the deque memory\n",
        "        minibatch = random.sample(memory, min(len(memory), batch_size))\n",
        "\n",
        "        state = np.zeros([batch_size,4])\n",
        "        next_state = np.zeros([batch_size,4])\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        # do this before prediction\n",
        "        # for speedup, this could be done on the tensor level\n",
        "        # but easier to understand using a loop       \n",
        "        for i in range(len(minibatch)):\n",
        "            state[i] = minibatch[i][0]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            done.append(minibatch[i][4])\n",
        "        # do batch prediction to save speed\n",
        "        # predict Q-values for starting state using the main network\n",
        "        target = model(torch.from_numpy(state).float().to(device)).detach()\n",
        "        target_old = target.clone()\n",
        "        # predict best action in ending state using the main network\n",
        "        target_next = model(torch.from_numpy(next_state).float().to(device)).detach()\n",
        "        # predict Q-values for ending state using the target network\n",
        "        target_val = target_model(torch.from_numpy(next_state).float().to(device)).detach()\n",
        "        for i in range(len(minibatch)):\n",
        "            # correction on the Q value for the action used\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                # the key point of Double DQN\n",
        "                # selection of action is from model\n",
        "                # update is from target model\n",
        "                if ddqn: # Double - DQN\n",
        "                    # current Q Network selects the action\n",
        "                    # a'_max = argmax_a' Q(s', a')\n",
        "                    a =  int(torch.argmax(target_next[i]))# np.argmax(target_next[i])\n",
        "                    # target Q Network evaluates the action\n",
        "                    # Q_max = Q_target(s', a'_max)\n",
        "                    target[i][action[i]] = reward[i] + gamma * (target_val[i][a])\n",
        "                else: # Standard - DQN\n",
        "                    # DQN chooses the max Q value among next actions\n",
        "                    # selection and evaluation of action is on the target Q Network\n",
        "                    # Q_max = max_a' Q_target(s', a')\n",
        "                    target[i][action[i]] = reward[i] + gamma * target_next[i].max(0)[0].detach()\n",
        "\n",
        "            \n",
        "                \n",
        "        # Train the Neural Network with batches\n",
        "        # self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "        pred = model(torch.from_numpy(state).float().to(device))\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_mcSlRt0Zb"
      },
      "source": [
        "Une dernière brique nécessaire à notre schéma d'apprentissage est de mettre en place la stratégie fixed target\n",
        "\n",
        "### Coding Task\n",
        "\n",
        "Compléter le code suivant pour implémenter la mise à jour des paramètres du réseaux cible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hZM-O8QzRK"
      },
      "source": [
        "def update_target_model(Soft_Update,ddqn,model,target_model,TAU):\n",
        "        if not Soft_Update:\n",
        "            \n",
        "            target_model.load_state_dict(model.state_dict()) \n",
        "            return\n",
        "        if Soft_Update and ddqn:    \n",
        "            with torch.no_grad():\n",
        "                #dict_model = dict(self.model.named_parameters())\n",
        "                #dict_target = dict(self.target.named_paramters())\n",
        "                for model, target in zip(self.model.named_parameters(), target_model.named_parameters()):\n",
        "                    model_name, model_weight = model\n",
        "                    target_name, target_weight = target\n",
        "                    #print(f'target_weight = {target_weight.data}')\n",
        "                    tmp = target_weight.data * (1-TAU) + model_weight.data * TAU\n",
        "                    target_weight.data.copy_(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLzC_S0QuJLI"
      },
      "source": [
        "Notre agent peut à présent être entraîné pour sa tâche. Vous trouverez dans le dossier Video, le films correspondant aux derniers épisodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c41PqydmXgxd"
      },
      "source": [
        "env_name = 'CartPole-v1'\n",
        "agent = DQNAgent(env_name, DQN1d, replay1d,update_target_model)\n",
        "agent.run1d()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts4oVHnCufVo"
      },
      "source": [
        "## Partie 2 : Duealing DQN\n",
        "\n",
        "Dans cette partie nous allons modifier l'architecture du réseau précédent pour permettre de choisir l'approche *duelling DQN*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cf. replay1d"
      ],
      "metadata": {
        "id": "DyyHM7HtRpMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mun69Q-3vMed"
      },
      "source": [
        "### Coding Task\n",
        "\n",
        "Compléter le modèle suivant pour construire $Q$ à partir de la fonction davantage $A$ et de $V$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzuOwW7u48P"
      },
      "source": [
        "# cf. Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_qzfSb8u5TA"
      },
      "source": [
        "Notre réseau peut être ensuite utilisé pour entraîner notre agent dans sa tâche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2P2Zi_ovB6X"
      },
      "source": [
        "env_name = 'CartPole-v1'\n",
        "agent = DQNAgent(env_name, DQN1d, replay1d,update_target_model,dueling=True)\n",
        "agent.run1d()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}