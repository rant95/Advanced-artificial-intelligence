{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCP211 : Policy gradient_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tNHoJ42UaU0"
      },
      "source": [
        "# TP 4 : Policy gradient\n",
        "\n",
        "Dans ce TP nous allons aborder les approches policy gradient au travers de l'algorithme REINFORCE pour bien comprendre la philosophie de ces approches. Dans une seconde partie nous prendrons également en main la bibliothèque *Stable Baseline 3* qui propose de nombreuses implémentations d'algorithmes de RL. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ58aUbZU244"
      },
      "source": [
        "## Partie 0 : Imports et dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WziTy5rBM0TO",
        "outputId": "fe7b8f8a-27de-40dd-9de9-f46e8c71430e"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/6ae6e774ac6cf8f5eeca1c30b9125231db901b75f72da7d81e939f293f69/stable_baselines3-1.0-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.8.1+cu101)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.2.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.30.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.3.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (56.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.4.1)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzLMvPOM03C"
      },
      "source": [
        "import os\n",
        "\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from gym.wrappers import Monitor\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import pylab as pl\n",
        "from IPython import display as ipdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNnuXa0sObmA"
      },
      "source": [
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiCyNx0oU70z"
      },
      "source": [
        "## Partie 1 : REINFORCE\n",
        "\n",
        "Dans cette partie nous allons coder l'algorithme REINFORCE pour entraîner un agent à maximiser ses récompense dans l'environnement de cartpole que nous avions déjà utilisé la dernière fois."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNVBIsx1VOA7"
      },
      "source": [
        "### Coding task\n",
        "\n",
        "Complétez le code suivant pour implémenter un réseaux de neuronnes chargé d'évaluer la politique à chaque instant.\n",
        "\n",
        "Ce réseau est composé de :\n",
        "* une couche linéaire de taille $(state\\_space\\times 128)$\n",
        "* une couche de dropout (p=0.6)\n",
        "* activation ReLU\n",
        "* une couche linéaire de taille $128\\times action\\_space)$\n",
        "* softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEJ6PU-OwOs"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(4, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, 2)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "\n",
        "\n",
        "policy = Policy()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSLXCJVZV60d"
      },
      "source": [
        "### Coding Task\n",
        "\n",
        "Complétez le code suivant pour tirer une action selon la distribution donné par le réseau `policy`. On pourra utiliser la fonction `Categorical()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za9s2LGmWK9n"
      },
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    probs = policy(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    policy.saved_log_probs.append(m.log_prob(action))\n",
        "    return action.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpBB4jW2WMQU"
      },
      "source": [
        "### Coding Task\n",
        "\n",
        "Complétez la fonction suivante pour implémenter le calcul des retours pour chaque étape de la trajectoire.\n",
        "\n",
        "1. Stocker les retours pour chaque étape de la trajectoire dans le tableau `returns`\n",
        "2. Sommer les scores $log \\; \\pi_\\theta(a|s)$ contenu dans le paramètre `saved_log_prob` du réseau fois le retour. On cherche donc à calculer notre fonction objectif :\n",
        "\n",
        "$$ V(\\theta) = \\sum_{t=0}^{T-1} \\log \\; \\pi_\\theta(a|s) G_t$$\n",
        "\n",
        "3. Calculer le gradient de l'objectif grâce à la fonction `backward` (ne pas oublier `optimizer.zero_grad()` et `optimizer.step`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkZDBJiCO6Zx"
      },
      "source": [
        "def update_policy(gamma):\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAuFhhJK0og1"
      },
      "source": [
        "On peut enfin entrâner notre agent. Les vidéos de l'entraînement sont disponibles dans le dossier *video*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZnzFCGNM8nO"
      },
      "source": [
        "running_reward = 10\n",
        "gamma = 0.99\n",
        "print_every = 10\n",
        "seed = np.random.randint(0,600)\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(seed)\n",
        "env = Monitor(env, './video', force=True)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "nb_epoch = 1e3\n",
        "nb_iter = 1e4\n",
        "\n",
        "for i_episode in range(nb_epoch):\n",
        "    state, ep_reward = env.reset(), 0\n",
        "    for t in range(1, nb_iter):  # Don't infinite loop while learning\n",
        "        action = select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        policy.rewards.append(reward)\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    update_policy(gamma)\n",
        "    if i_episode % print_every == 0:\n",
        "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "              i_episode, ep_reward, running_reward))\n",
        "    if running_reward > env.spec.reward_threshold:\n",
        "        print(\"Solved! Running reward is now {} and \"\n",
        "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgUzzWlWWoZk"
      },
      "source": [
        "## Partie 2 : RL avec Stable Baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLl4DCuvJIdu"
      },
      "source": [
        "Dans cette partie nous allons entraîner un Deep Q-Network (DQN) sur une tâche classique en RL : le cartpole. Il s'agit d'apprendre à un agent à tenir en équilibre un mat sur un véhicule 2D en mouvement. Les actions sont ${gauche,droite}$ et les états sont décrits par la position du véhicule, l'angle du mât et sa vélocité. L'environnement est présenté dans l'interface `gym` *cf.* [ici](https://gym.openai.com/envs/CartPole-v1/). Pour cette tâche nous allons utiliser la bibliothèque Standard Baseline 3 [site](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html) proposant un large panel d'algorithme de RL implémenté et le biding avec l'environnement `gym`. Nous pouvons donc prendre une approche relativement au niveau dans le sens où nous nous contenterons de choisir l'environnement (ici cartpole), la durée de l'entraînement donnée par le paramètre `total_timesteps` dans la méthode `learn()`. \n",
        "\n",
        "Commençons par installer les dépendences et définir quelques fonctins utilitaires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "y8vXWRvKMo_1"
      },
      "source": [
        "#@title Utilitaires\n",
        "\n",
        "import gym\n",
        "\n",
        "from gym.wrappers import Monitor\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import pylab as pl\n",
        "from IPython import display as ipdisplay\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def make_env(env_id, rank, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "\n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    set_random_seed(seed)\n",
        "    return _init\n",
        "\n",
        "\n",
        "\n",
        "def record(model,path,length):\n",
        "\n",
        "  video_folder = path\n",
        "  video_length = length\n",
        "\n",
        "  env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "\n",
        "\n",
        "  # Record the video starting at the first step\n",
        "  env = VecVideoRecorder(env, video_folder,\n",
        "                        record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
        "                        name_prefix=\"random-agent-{}\".format(env_id))\n",
        "\n",
        "  obs = env.reset()\n",
        "  for _ in range(video_length + 1):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "  # Save the video\n",
        "  env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rKAjpP2KcYj"
      },
      "source": [
        "À présent, nous pouvons entraîner un DQN dans l'environnement cartpole. Nous choisissons ici un Multi Layer Perceptron et un entraîenement de $10^5$ steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zipe6GZuXIfY"
      },
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "num_cpu = 1  # Number of processes to use\n",
        "# Create the vectorized environment\n",
        "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
        "\n",
        "model = DQN('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=1e5)\n",
        "\n",
        "model.save(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tELytXm4KvDa"
      },
      "source": [
        "Nous pouvons enregistrer une vidéo des performances de l'agent de la durée `length`. Cette vidéo sera stockée dans le répertoire donné en chemin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giqo525ZXLXN"
      },
      "source": [
        "record(model,'logs/videos',length=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usp7gAfNXdHL"
      },
      "source": [
        "### Coding Task\n",
        "\n",
        "Reprenez les fonctions précédentes pour entrainer un agent avec l'algorithme Avantage Actor Critic (A2C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epzTpP5SXndG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DT2uGmqXoUR"
      },
      "source": [
        "### Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JpoEOfXK-sA"
      },
      "source": [
        "Vous pouvez tester cette approche sur des jeux Atari (pour cela vous devez télécharger quelques fichiers de configuration *cf.* cellule suivante). Attention, ces environnemnets sont plus complexes et nécessitent plus temps d'entraînement voire une approche différente de DQN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUO8092tXt3K"
      },
      "source": [
        "#@title Atari data\n",
        "\n",
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar e Roms.rar\n",
        "!unzip HC\\ ROMS.zip\n",
        "!unzip ROMS.zip\n",
        "!python -m atari_py.import_roms ROMS"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}